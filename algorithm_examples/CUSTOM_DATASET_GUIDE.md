# Adding Custom Datasets to PilotScope

## Overview

PilotScopeì˜ ë°ì´í„°ì…‹ ì‹œìŠ¤í…œì€ **3ê°€ì§€ êµ¬ì„± ìš”ì†Œ**ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤:

1. **SQL íŒŒì¼** (`.txt`): í•™ìŠµ/í…ŒìŠ¤íŠ¸ìš© SQL ì¿¼ë¦¬ ëª¨ìŒ
2. **ë°ì´í„°ì…‹ í´ë˜ìŠ¤** (`.py`): SQL ë¡œë”© ë° DB ì´ˆê¸°í™” ë¡œì§
3. **ë°ì´í„° íŒŒì¼** (optional): ì‹¤ì œ ë°ì´í„° (dump íŒŒì¼, CSV ë“±)

---

## Quick Start: SQL íŒŒì¼ë§Œ ì¶”ê°€í•˜ê¸° (ê°€ì¥ ê°„ë‹¨)

ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ìƒˆë¡œìš´ SQL ì›Œí¬ë¡œë“œë§Œ ì¶”ê°€í•˜ë ¤ë©´:

### Step 1: SQL íŒŒì¼ ìƒì„±

```bash
# pilotscope/Dataset/MyWorkload/ í´ë” ìƒì„±
mkdir pilotscope/Dataset/MyWorkload

# SQL íŒŒì¼ ì‘ì„±
# pilotscope/Dataset/MyWorkload/my_train.txt
```

**í˜•ì‹** (ê° ì¿¼ë¦¬ëŠ” ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„):
```sql
select count(*) from users where age > 25;
select name, email from users where created_at > '2024-01-01'::timestamp;
select u.name, count(o.id) from users u join orders o on u.id = o.user_id group by u.name;
```

### Step 2: ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìƒì„±

```python
# pilotscope/Dataset/MyWorkloadDataset.py
from pilotscope.Dataset.BaseDataset import BaseDataset
from pilotscope.PilotEnum import DatabaseEnum
import os

class MyWorkloadDataset(BaseDataset):
    """
    Custom workload for my application.
    """
    sub_dir = "MyWorkload"
    train_sql_file = "my_train.txt"
    test_sql_file = "my_test.txt"
    file_db_type = DatabaseEnum.POSTGRESQL  # SQL íŒŒì¼ì˜ ë¬¸ë²• íƒ€ì…
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="my_database", data_dir=None):
        super().__init__(use_db_type, created_db_name, data_dir)
        # download_urlsëŠ” None (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” DB ì‚¬ìš©)
        self.download_urls = None
```

### Step 3: ì‚¬ìš©í•˜ê¸°

```python
from pilotscope.Dataset.MyWorkloadDataset import MyWorkloadDataset
from pilotscope.PilotEnum import DatabaseEnum

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = MyWorkloadDataset(DatabaseEnum.POSTGRESQL, created_db_name="production_db")

# SQL ê°€ì ¸ì˜¤ê¸°
train_sqls = dataset.read_train_sql()
test_sqls = dataset.read_test_sql()

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
for sql in train_sqls:
    result = scheduler.execute(sql)
```

---

## Advanced: ì™„ì „í•œ ë°ì´í„°ì…‹ ì¶”ê°€í•˜ê¸°

ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ + ë°ì´í„° + SQLì„ ëª¨ë‘ í¬í•¨í•˜ë ¤ë©´:

### Directory Structure

```
pilotscope/Dataset/
â”œâ”€â”€ MyDataset/                    # ìƒˆ ë°ì´í„°ì…‹ í´ë”
â”‚   â”œâ”€â”€ schema.sql               # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜
â”‚   â”œâ”€â”€ my_train.txt             # í•™ìŠµìš© SQL
â”‚   â””â”€â”€ my_test.txt              # í…ŒìŠ¤íŠ¸ìš© SQL
â””â”€â”€ MyDatasetDataset.py          # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
```

### Step 1: ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì‘ì„±

```sql
-- pilotscope/Dataset/MyDataset/schema.sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    age INT,
    created_at TIMESTAMP
);

CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    amount DECIMAL(10,2),
    created_at TIMESTAMP
);

CREATE INDEX idx_users_age ON users(age);
CREATE INDEX idx_orders_user_id ON orders(user_id);
```

### Step 2: SQL ì›Œí¬ë¡œë“œ ì‘ì„±

```sql
-- pilotscope/Dataset/MyDataset/my_train.txt
select count(*) from users where age > 25;
select name from users where email like '%@gmail.com';
select u.name, count(o.id) from users u join orders o on u.id = o.user_id group by u.name;
```

### Step 3: ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì‘ì„±

```python
# pilotscope/Dataset/MyDatasetDataset.py
from pilotscope.Dataset.BaseDataset import BaseDataset
from pilotscope.PilotEnum import DatabaseEnum
import os

class MyDatasetDataset(BaseDataset):
    """
    My custom dataset with schema and data.
    """
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ URL (GitHub Release ë“±)
    data_location_dict = {
        DatabaseEnum.POSTGRESQL: [
            "https://github.com/myuser/myrepo/releases/download/v1.0/my_data.tar.gz"
        ],
        DatabaseEnum.SPARK: None
    }
    
    # ë°ì´í„° íŒŒì¼ì˜ SHA256 í•´ì‹œ (ë¬´ê²°ì„± ê²€ì¦ìš©)
    data_sha256 = "your_sha256_hash_here"
    
    # íŒŒì¼ ê²½ë¡œ
    sub_dir = "MyDataset"
    schema_file = "schema.sql"
    train_sql_file = "my_train.txt"
    test_sql_file = "my_test.txt"
    file_db_type = DatabaseEnum.POSTGRESQL
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="my_db", data_dir=None):
        super().__init__(use_db_type, created_db_name, data_dir)
        self.download_urls = self.data_location_dict[use_db_type]
```

### Step 4: ë°ì´í„° ì¤€ë¹„ (Optional)

ë°ì´í„° íŒŒì¼ì„ ì œê³µí•˜ë ¤ë©´:

```bash
# PostgreSQL dump íŒŒì¼ ìƒì„±
pg_dump my_database > my_data.dump

# tar.gzë¡œ ì••ì¶•
tar -czf my_data.tar.gz my_data.dump

# GitHub Release ë“±ì— ì—…ë¡œë“œí•˜ê³  URLì„ data_location_dictì— ì¶”ê°€
```

### Step 5: ì‚¬ìš©í•˜ê¸°

```python
from pilotscope.PilotConfig import PostgreSQLConfig
from pilotscope.Dataset.MyDatasetDataset import MyDatasetDataset
from pilotscope.PilotEnum import DatabaseEnum

# 1. ë°ì´í„°ì…‹ ì´ˆê¸°í™”
dataset = MyDatasetDataset(DatabaseEnum.POSTGRESQL)

# 2. ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ (ìµœì´ˆ 1íšŒ)
config = PostgreSQLConfig()
dataset.load_to_db(config)  # ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ + DB ìƒì„±

# 3. ìŠ¤í‚¤ë§ˆ ì‹¤í–‰ (í•„ìš”ì‹œ)
schema_sqls = dataset.read_schema()
for sql in schema_sqls:
    db_controller.execute(sql)

# 4. ì›Œí¬ë¡œë“œ ì‹¤í–‰
train_sqls = dataset.read_train_sql()
test_sqls = dataset.read_test_sql()
```

---

## Pattern 1: ê¸°ì¡´ í”„ë¡œë•ì…˜ DBì˜ ì¿¼ë¦¬ ë¡œê·¸ ì‚¬ìš©

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤:

```python
# pilotscope/Dataset/ProductionWorkloadDataset.py
from pilotscope.Dataset.BaseDataset import BaseDataset
from pilotscope.PilotEnum import DatabaseEnum

class ProductionWorkloadDataset(BaseDataset):
    """
    Real production query logs from our application.
    """
    sub_dir = "ProductionWorkload"
    train_sql_file = "prod_queries_2024_01.txt"  # 1ì›” ì¿¼ë¦¬
    test_sql_file = "prod_queries_2024_02.txt"   # 2ì›” ì¿¼ë¦¬
    file_db_type = DatabaseEnum.POSTGRESQL
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="production"):
        super().__init__(use_db_type, created_db_name, data_dir=None)
        self.download_urls = None  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” DB ì‚¬ìš©

# ì‚¬ìš©
dataset = ProductionWorkloadDataset(DatabaseEnum.POSTGRESQL)
real_queries = dataset.read_train_sql()
```

**ì¿¼ë¦¬ ë¡œê·¸ ìˆ˜ì§‘ ë°©ë²•:**
```sql
-- PostgreSQL ì¿¼ë¦¬ ë¡œê·¸ í™œì„±í™”
ALTER SYSTEM SET log_statement = 'all';
SELECT pg_reload_conf();

-- ë¡œê·¸ì—ì„œ SQL ì¶”ì¶œ
grep "SELECT\|INSERT\|UPDATE\|DELETE" /var/log/postgresql/postgresql.log > prod_queries.txt
```

---

## Pattern 2: ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ì›Œí¬ë¡œë“œ

íŠ¹ì • ê¸°ëŠ¥ì˜ ì¿¼ë¦¬ë§Œ ëª¨ì•„ì„œ:

```python
# pilotscope/Dataset/AnalyticsWorkloadDataset.py
class AnalyticsWorkloadDataset(BaseDataset):
    """
    Analytics dashboard queries only.
    """
    sub_dir = "AnalyticsWorkload"
    train_sql_file = "analytics_train.txt"
    test_sql_file = "analytics_test.txt"
    file_db_type = DatabaseEnum.POSTGRESQL
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="analytics_db"):
        super().__init__(use_db_type, created_db_name)
        self.download_urls = None

# analytics_train.txt ì˜ˆì‹œ:
"""
select date_trunc('day', created_at) as date, count(*) from orders group by date;
select category, sum(amount) from products p join orders o on p.id = o.product_id group by category;
select user_id, count(*) as order_count from orders where created_at > '2024-01-01' group by user_id;
"""
```

---

## Pattern 3: ë™ì  SQL ìƒì„±

íŒŒì¼ ëŒ€ì‹  ì½”ë“œë¡œ SQL ìƒì„±:

```python
# pilotscope/Dataset/SyntheticWorkloadDataset.py
from pilotscope.Dataset.BaseDataset import BaseDataset
import random

class SyntheticWorkloadDataset(BaseDataset):
    """
    Dynamically generated synthetic workload.
    """
    def __init__(self, use_db_type, created_db_name="test_db"):
        super().__init__(use_db_type, created_db_name)
    
    def read_train_sql(self):
        """ë™ì ìœ¼ë¡œ SQL ìƒì„±"""
        sqls = []
        for i in range(100):
            age = random.randint(18, 80)
            sqls.append(f"select count(*) from users where age > {age}")
        return sqls
    
    def read_test_sql(self):
        """í…ŒìŠ¤íŠ¸ìš© SQL ìƒì„±"""
        sqls = []
        for i in range(50):
            limit = random.randint(10, 100)
            sqls.append(f"select * from users order by created_at desc limit {limit}")
        return sqls

# ì‚¬ìš©
dataset = SyntheticWorkloadDataset(DatabaseEnum.POSTGRESQL, "my_db")
train_sqls = dataset.read_train_sql()  # ë§¤ë²ˆ ë‹¤ë¥¸ ì¿¼ë¦¬ ìƒì„±
```

---

## Utility: utils.py ìˆ˜ì •

ê¸°ì¡´ `algorithm_examples/utils.py`ì˜ `load_test_sql()` í•¨ìˆ˜ë¥¼ í™•ì¥:

```python
# algorithm_examples/utils.pyì— ì¶”ê°€
from pilotscope.Dataset.MyWorkloadDataset import MyWorkloadDataset

def load_test_sql(db):
    if "stats_tiny" == db.lower():
        return StatsTinyDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    elif "stats" in db.lower():
        return StatsDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    elif "imdb" in db:
        return ImdbDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    elif "tpcds" in db.lower():
        return TpcdsDataset(DatabaseEnum).read_test_sql()
    elif "my_workload" == db.lower():  # ì¶”ê°€!
        return MyWorkloadDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    elif "production" == db.lower():   # ì¶”ê°€!
        return ProductionWorkloadDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    else:
        raise NotImplementedError
```

---

## SQL íŒŒì¼ í˜•ì‹ ê·œì¹™

### âœ… ì˜¬ë°”ë¥¸ í˜•ì‹

```sql
select count(*) from users where age > 25;
select name from users where email like '%@gmail.com';
select u.name, count(o.id) from users u join orders o on u.id = o.user_id group by u.name;
```

**ê·œì¹™:**
- ê° ì¿¼ë¦¬ëŠ” **ì„¸ë¯¸ì½œë¡ (`;`)ìœ¼ë¡œ ì¢…ë£Œ**
- í•œ ì¤„ ë˜ëŠ” ì—¬ëŸ¬ ì¤„ ëª¨ë‘ ê°€ëŠ¥
- ì£¼ì„ ê°€ëŠ¥ (`--` ë˜ëŠ” `/* */`)

### âŒ ì˜ëª»ëœ í˜•ì‹

```sql
select count(*) from users where age > 25  # ì„¸ë¯¸ì½œë¡  ì—†ìŒ âŒ
select name from users;; # ì„¸ë¯¸ì½œë¡  2ê°œ âŒ
```

---

## Complete Example: E-commerce Dataset

ì‹¤ì œ ì „ììƒê±°ë˜ ë°ì´í„°ì…‹ ì˜ˆì œ:

```python
# pilotscope/Dataset/EcommerceDataset.py
from pilotscope.Dataset.BaseDataset import BaseDataset
from pilotscope.PilotEnum import DatabaseEnum

class EcommerceDataset(BaseDataset):
    """
    E-commerce workload with orders, products, and users.
    """
    sub_dir = "Ecommerce"
    schema_file = "ecommerce_schema.sql"
    train_sql_file = "ecommerce_train.txt"
    test_sql_file = "ecommerce_test.txt"
    file_db_type = DatabaseEnum.POSTGRESQL
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="ecommerce"):
        super().__init__(use_db_type, created_db_name)
        self.download_urls = None

# í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©
from pilotscope.PilotConfig import PostgreSQLConfig

config = PostgreSQLConfig()
config.db = "ecommerce"

dataset = EcommerceDataset(DatabaseEnum.POSTGRESQL)
train_sqls = dataset.read_train_sql()

# ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
scheduler = get_mscn_preset_scheduler(config)
for sql in train_sqls:
    scheduler.execute(sql)
```

---

## Summary

### ğŸ¯ 3ê°€ì§€ ë°©ë²•

| ë°©ë²• | ì–¸ì œ ì‚¬ìš©? | í•„ìš” íŒŒì¼ |
|------|-----------|----------|
| **SQLë§Œ ì¶”ê°€** | ê¸°ì¡´ DBì— ìƒˆ ì›Œí¬ë¡œë“œ ì¶”ê°€ | `.txt` íŒŒì¼ë§Œ |
| **ì™„ì „í•œ ë°ì´í„°ì…‹** | ìƒˆ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë°°í¬ | `.txt` + `schema.sql` + ë°ì´í„° |
| **ë™ì  ìƒì„±** | í•©ì„± ì›Œí¬ë¡œë“œ, í…ŒìŠ¤íŠ¸ | Python ì½”ë“œë¡œ ìƒì„± |

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `pilotscope/Dataset/{MyDataset}/` í´ë” ìƒì„±
- [ ] `{dataset}_train.txt`, `{dataset}_test.txt` ì‘ì„±
- [ ] `MyDatasetDataset.py` í´ë˜ìŠ¤ ì‘ì„±
- [ ] `algorithm_examples/utils.py`ì— ë¡œë”© ë¡œì§ ì¶”ê°€
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í™•ì¸

### ğŸš€ Quick Template

```bash
# 1. í´ë” ìƒì„±
mkdir pilotscope/Dataset/MyDataset

# 2. SQL íŒŒì¼ ì‘ì„±
echo "select * from users;" > pilotscope/Dataset/MyDataset/my_train.txt

# 3. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë³µì‚¬ & ìˆ˜ì •
cp pilotscope/Dataset/StatsDataset.py pilotscope/Dataset/MyDatasetDataset.py
# sub_dir, train_sql_file ë“± ìˆ˜ì •

# 4. ì‚¬ìš©!
python test_example_algorithms/test_mscn_example.py
```

**í•µì‹¬**: `.txt` íŒŒì¼ì— SQLì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì €ì¥í•˜ë©´ ë! ğŸ‰
