# ê°œì„ ëœ ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ: íƒ€ì„ìŠ¤íƒ¬í”„ + ë©”íƒ€ë°ì´í„°

> "ì„ì‹œ ì €ì¥ ëŠë‚Œ"ìœ¼ë¡œ ë¶€ë‹´ ì—†ì´ ì‹¤í—˜í•˜ê³ , ë‚˜ì¤‘ì— ìµœì  ëª¨ë¸ë§Œ ì„ íƒ

---

## ğŸ¯ ì„¤ê³„ ì›ì¹™

1. **íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì €ì¥**: ì´ë¦„ ì¶©ëŒ ì—†ìŒ, ë¶€ë‹´ ì—†ì´ ì €ì¥
2. **í’ë¶€í•œ ë©”íƒ€ë°ì´í„°**: ëª¨ë“  ë³€ìˆ˜ (íŒŒë¼ë¯¸í„°, ë°ì´í„°ì…‹, ì„±ëŠ¥) ê¸°ë¡
3. **Train/Test ë¶„ë¦¬**: í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬
4. **ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¡œ ê´€ë¦¬**: ìµœì  ëª¨ë¸ ì°¾ê¸°, ë¹„êµ, ì •ë¦¬

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
ExampleData/
â”œâ”€â”€ Mscn/
â”‚   â”œâ”€â”€ Model/
â”‚   â”‚   â”œâ”€â”€ mscn_20241019_103000          # íƒ€ì„ìŠ¤íƒ¬í”„ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ mscn_20241019_103000.json     # ë©”íƒ€ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ mscn_20241019_150000
â”‚   â”‚   â”œâ”€â”€ mscn_20241019_150000.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ training_registry.json            # í•™ìŠµ ì´ë ¥
â”œâ”€â”€ Lero/
â”‚   â”œâ”€â”€ Model/
â”‚   â”‚   â”œâ”€â”€ lero_20241019_110000
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ training_registry.json
â””â”€â”€ model_registry.json                   # ì „ì²´ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
```

---

## ğŸ’¾ ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ

```json
{
  "model_id": "mscn_20241019_103000",
  "algorithm": "mscn",
  "model_path": "ExampleData/Mscn/Model/mscn_20241019_103000",
  
  "training": {
    "enabled": true,
    "dataset": "production",
    "num_queries": 500,
    "hyperparams": {
      "num_epoch": 100,
      "num_training": 500,
      "num_collection": 500,
      "learning_rate": 0.001
    },
    "trained_at": "2024-10-19T10:30:00",
    "training_time": 1800.5
  },
  
  "testing": [
    {
      "dataset": "production",
      "num_queries": 100,
      "tested_at": "2024-10-19T12:00:00",
      "performance": {
        "total_time": 45.23,
        "average_time": 0.45,
        "median_time": 0.38,
        "p95_time": 1.2
      }
    },
    {
      "dataset": "stats_tiny",
      "num_queries": 80,
      "tested_at": "2024-10-19T13:00:00",
      "performance": {
        "total_time": 35.12,
        "average_time": 0.44
      }
    }
  ],
  
  "tags": ["production", "experiment", "best"],
  "notes": "Production ë°ì´í„°ë¡œ í•™ìŠµ, ì„±ëŠ¥ ì¢‹ìŒ"
}
```

---

## ğŸ”§ êµ¬í˜„: Enhanced PilotModel

```python
# algorithm_examples/enhanced_pilot_model.py

import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from pilotscope.PilotModel import PilotModel


class EnhancedPilotModel(PilotModel):
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ëª¨ë¸ ì €ì¥ + í’ë¶€í•œ ë©”íƒ€ë°ì´í„°
    """
    
    def __init__(self, model_name, algorithm_type):
        super().__init__(model_name)
        self.algorithm_type = algorithm_type  # "mscn", "lero", etc.
        self.model_save_dir = f"../algorithm_examples/ExampleData/{algorithm_type.capitalize()}/Model"
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.model_id = f"{model_name}_{timestamp}"
        self.model_path = os.path.join(self.model_save_dir, self.model_id)
        self.metadata_path = self.model_path + ".json"
        
        # ë©”íƒ€ë°ì´í„°
        self.metadata = {
            "model_id": self.model_id,
            "algorithm": self.algorithm_type,
            "model_path": self.model_path,
            "training": None,
            "testing": [],
            "tags": [],
            "notes": ""
        }
    
    def set_training_info(self, dataset: str, hyperparams: Dict, 
                         num_queries: int = None):
        """
        í•™ìŠµ ì •ë³´ ì„¤ì •
        
        Args:
            dataset: í•™ìŠµ ë°ì´í„°ì…‹ ì´ë¦„
            hyperparams: í•˜ì´í¼íŒŒë¼ë¯¸í„°
            num_queries: í•™ìŠµì— ì‚¬ìš©í•œ ì¿¼ë¦¬ ìˆ˜
        """
        self.metadata["training"] = {
            "enabled": True,
            "dataset": dataset,
            "num_queries": num_queries,
            "hyperparams": hyperparams,
            "trained_at": datetime.now().isoformat(),
            "training_time": None  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
        }
    
    def add_test_result(self, dataset: str, num_queries: int, 
                       performance: Dict):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€ (ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•´ ëˆ„ì  ê°€ëŠ¥)
        
        Args:
            dataset: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            num_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜
            performance: ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        test_result = {
            "dataset": dataset,
            "num_queries": num_queries,
            "tested_at": datetime.now().isoformat(),
            "performance": performance
        }
        self.metadata["testing"].append(test_result)
    
    def add_tags(self, *tags):
        """íƒœê·¸ ì¶”ê°€ (ì˜ˆ: "production", "best", "experiment")"""
        self.metadata["tags"].extend(tags)
    
    def set_notes(self, notes: str):
        """ë©”ëª¨ ì¶”ê°€"""
        self.metadata["notes"] = notes
    
    def save_model(self):
        """ëª¨ë¸ + ë©”íƒ€ë°ì´í„° ì €ì¥"""
        import time
        start_time = time.time()
        
        # 1. ëª¨ë¸ ì €ì¥ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        self._save_model_impl()
        
        # 2. í•™ìŠµ ì‹œê°„ ì—…ë°ì´íŠ¸
        if self.metadata["training"]:
            self.metadata["training"]["training_time"] = time.time() - start_time
        
        # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
        os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Model saved: {self.model_path}")
        print(f"âœ… Metadata saved: {self.metadata_path}")
    
    def _save_model_impl(self):
        """ì‹¤ì œ ëª¨ë¸ ì €ì¥ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    @classmethod
    def load_model(cls, model_id: str, algorithm_type: str):
        """
        íŠ¹ì • ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_id: íƒ€ì„ìŠ¤íƒ¬í”„ ID (ì˜ˆ: "mscn_20241019_103000")
            algorithm_type: ì•Œê³ ë¦¬ì¦˜ íƒ€ì…
        """
        model_save_dir = f"../algorithm_examples/ExampleData/{algorithm_type.capitalize()}/Model"
        model_path = os.path.join(model_save_dir, model_id)
        metadata_path = model_path + ".json"
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = cls(model_id.split('_')[0], algorithm_type)
        instance.model_id = model_id
        instance.model_path = model_path
        instance.metadata = metadata
        
        # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
        instance._load_model_impl()
        
        print(f"âœ… Model loaded: {model_id}")
        print(f"   Training: {metadata['training']['dataset'] if metadata['training'] else 'N/A'}")
        print(f"   Tests: {len(metadata['testing'])}")
        
        return instance
    
    def _load_model_impl(self):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë“œ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError


# MSCN êµ¬í˜„
from algorithm_examples.Mscn.source.mscn_model import MscnModel

class MscnPilotModel(EnhancedPilotModel):
    """MSCNìš© Enhanced Model"""
    
    def __init__(self, model_name="mscn"):
        super().__init__(model_name, "mscn")
    
    def _save_model_impl(self):
        """MSCN ëª¨ë¸ ì €ì¥"""
        self.model.save(self.model_path)
    
    def _load_model_impl(self):
        """MSCN ëª¨ë¸ ë¡œë“œ"""
        try:
            model = MscnModel()
            model.load(self.model_path)
        except:
            print("âš ï¸  Model file not found, creating new model")
            model = MscnModel()
        self.model = model


# Lero êµ¬í˜„
from algorithm_examples.Lero.source.model import LeroModelPairWise

class LeroPilotModel(EnhancedPilotModel):
    """Leroìš© Enhanced Model"""
    
    def __init__(self, model_name="lero"):
        super().__init__(model_name, "lero")
    
    def _save_model_impl(self):
        """Lero ëª¨ë¸ ì €ì¥"""
        self.model.save(self.model_path)
    
    def _load_model_impl(self):
        """Lero ëª¨ë¸ ë¡œë“œ"""
        try:
            model = LeroModelPairWise(None)
            model.load(self.model_path)
        except FileNotFoundError:
            print("âš ï¸  Model file not found, creating new model")
            model = LeroModelPairWise(None)
        self.model = model
```

---

## ğŸ—‚ï¸ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

```python
# algorithm_examples/model_registry.py

import json
import os
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime


class ModelRegistry:
    """
    ëª¨ë¸ ê´€ë¦¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬
    - ëª¨ë¸ ì¡°íšŒ
    - ìµœì  ëª¨ë¸ ì°¾ê¸°
    - ëª¨ë¸ ë¹„êµ
    - ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬
    """
    
    def __init__(self, registry_file="algorithm_examples/ExampleData/model_registry.json"):
        self.registry_file = registry_file
        self._ensure_registry_exists()
    
    def _ensure_registry_exists(self):
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒŒì¼ ìƒì„±"""
        if not os.path.exists(self.registry_file):
            os.makedirs(os.path.dirname(self.registry_file), exist_ok=True)
            with open(self.registry_file, 'w') as f:
                json.dump({}, f)
    
    def _load_registry(self) -> Dict:
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ"""
        with open(self.registry_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _save_registry(self, registry: Dict):
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥"""
        with open(self.registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, indent=2, ensure_ascii=False)
    
    def register_model(self, metadata: Dict):
        """ëª¨ë¸ ë“±ë¡"""
        registry = self._load_registry()
        model_id = metadata["model_id"]
        registry[model_id] = metadata
        self._save_registry(registry)
        print(f"âœ… Model registered: {model_id}")
    
    def list_models(self, algorithm: str = None, 
                   dataset: str = None,
                   tags: List[str] = None,
                   sort_by: str = "trained_at") -> List[Dict]:
        """
        ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        
        Args:
            algorithm: ì•Œê³ ë¦¬ì¦˜ í•„í„°
            dataset: í•™ìŠµ ë°ì´í„°ì…‹ í•„í„°
            tags: íƒœê·¸ í•„í„°
            sort_by: ì •ë ¬ ("trained_at", "performance")
        """
        registry = self._load_registry()
        models = []
        
        for model_id, metadata in registry.items():
            # í•„í„°ë§
            if algorithm and metadata.get("algorithm") != algorithm:
                continue
            
            if dataset:
                training = metadata.get("training", {})
                if training.get("dataset") != dataset:
                    continue
            
            if tags:
                model_tags = set(metadata.get("tags", []))
                if not set(tags).issubset(model_tags):
                    continue
            
            models.append(metadata)
        
        # ì •ë ¬
        if sort_by == "trained_at":
            models.sort(key=lambda x: x.get("training", {}).get("trained_at", ""), 
                       reverse=True)
        elif sort_by == "performance":
            def get_avg_performance(m):
                tests = m.get("testing", [])
                if not tests:
                    return float('inf')
                avg_time = sum(t["performance"].get("total_time", 0) for t in tests) / len(tests)
                return avg_time
            models.sort(key=get_avg_performance)
        
        return models
    
    def get_best_model(self, algorithm: str, 
                      dataset: str = None,
                      metric: str = "total_time") -> Optional[Dict]:
        """
        ìµœì  ëª¨ë¸ ì¡°íšŒ
        
        Args:
            algorithm: ì•Œê³ ë¦¬ì¦˜
            dataset: íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸ë§Œ
            metric: ìµœì í™” ê¸°ì¤€
        """
        models = self.list_models(algorithm=algorithm)
        
        if not models:
            return None
        
        # ê° ëª¨ë¸ì˜ íŠ¹ì • ë°ì´í„°ì…‹ ì„±ëŠ¥ ì¶”ì¶œ
        candidates = []
        for model in models:
            for test in model.get("testing", []):
                if dataset and test["dataset"] != dataset:
                    continue
                
                perf = test["performance"].get(metric)
                if perf is not None:
                    candidates.append({
                        "model": model,
                        "performance": perf,
                        "test_dataset": test["dataset"]
                    })
        
        if not candidates:
            return None
        
        # ìµœì†Œê°’ (ì‹œê°„ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        best = min(candidates, key=lambda x: x["performance"])
        return best["model"]
    
    def compare_models(self, model_ids: List[str], 
                      test_dataset: str = None) -> None:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ì¶œë ¥
        
        Args:
            model_ids: ë¹„êµí•  ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸
            test_dataset: íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€
        """
        registry = self._load_registry()
        
        print("\n" + "="*100)
        print("Model Comparison")
        print("="*100)
        print(f"{'Model ID':<30} {'Train Dataset':<15} {'Test Dataset':<15} "
              f"{'Total Time':<12} {'Avg Time':<10} {'Epochs':<8}")
        print("-"*100)
        
        for model_id in model_ids:
            metadata = registry.get(model_id)
            if not metadata:
                continue
            
            # í•™ìŠµ ì •ë³´
            training = metadata.get("training", {})
            train_dataset = training.get("dataset", "N/A")
            epochs = training.get("hyperparams", {}).get("num_epoch", "N/A")
            
            # í…ŒìŠ¤íŠ¸ ì •ë³´
            tests = metadata.get("testing", [])
            if test_dataset:
                tests = [t for t in tests if t["dataset"] == test_dataset]
            
            if not tests:
                print(f"{model_id:<30} {train_dataset:<15} {'No tests':<15} "
                      f"{'-':<12} {'-':<10} {epochs:<8}")
            else:
                for test in tests:
                    perf = test["performance"]
                    total_time = perf.get("total_time", 0)
                    avg_time = perf.get("average_time", 0)
                    test_ds = test["dataset"]
                    
                    print(f"{model_id:<30} {train_dataset:<15} {test_ds:<15} "
                          f"{total_time:<12.2f} {avg_time:<10.4f} {epochs:<8}")
        
        print("="*100)
    
    def cleanup_old_models(self, algorithm: str, keep_top_n: int = 5,
                          by_metric: str = "total_time") -> List[str]:
        """
        ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬ (ìƒìœ„ Nê°œë§Œ ìœ ì§€)
        
        Args:
            algorithm: ì•Œê³ ë¦¬ì¦˜
            keep_top_n: ìœ ì§€í•  ëª¨ë¸ ê°œìˆ˜
            by_metric: ì •ë ¬ ê¸°ì¤€
        
        Returns:
            ì‚­ì œëœ ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸
        """
        models = self.list_models(algorithm=algorithm, sort_by="performance")
        
        if len(models) <= keep_top_n:
            print(f"â„¹ï¸  Only {len(models)} models, no cleanup needed")
            return []
        
        # í•˜ìœ„ ëª¨ë¸ ì‚­ì œ
        to_delete = models[keep_top_n:]
        deleted_ids = []
        
        registry = self._load_registry()
        
        for model in to_delete:
            model_id = model["model_id"]
            model_path = model["model_path"]
            
            # íŒŒì¼ ì‚­ì œ
            try:
                if os.path.exists(model_path):
                    os.remove(model_path)
                metadata_path = model_path + ".json"
                if os.path.exists(metadata_path):
                    os.remove(metadata_path)
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì œê±°
                del registry[model_id]
                deleted_ids.append(model_id)
                print(f"ğŸ—‘ï¸  Deleted: {model_id}")
            except Exception as e:
                print(f"âš ï¸  Failed to delete {model_id}: {e}")
        
        self._save_registry(registry)
        print(f"\nâœ… Cleanup complete: {len(deleted_ids)} models deleted")
        
        return deleted_ids
    
    def tag_model(self, model_id: str, *tags):
        """ëª¨ë¸ì— íƒœê·¸ ì¶”ê°€"""
        registry = self._load_registry()
        
        if model_id in registry:
            existing_tags = set(registry[model_id].get("tags", []))
            existing_tags.update(tags)
            registry[model_id]["tags"] = list(existing_tags)
            self._save_registry(registry)
            print(f"âœ… Tags added to {model_id}: {tags}")
        else:
            print(f"âŒ Model not found: {model_id}")
```

---

## ğŸ”— PresetScheduler í†µí•©

```python
# algorithm_examples/Mscn/MscnPresetScheduler.py (ìˆ˜ì •)

from algorithm_examples.enhanced_pilot_model import MscnPilotModel
from algorithm_examples.model_registry import ModelRegistry

def get_mscn_preset_scheduler(config, enable_collection, enable_training,
                             num_collection=-1, num_training=-1, num_epoch=100,
                             load_model_id=None):  # ì¶”ê°€!
    """
    Args:
        load_model_id: íŠ¹ì • ëª¨ë¸ ë¡œë“œ (Noneì´ë©´ ìƒˆ ëª¨ë¸ ë˜ëŠ” ìµœì  ëª¨ë¸)
    """
    
    # 1. ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    if load_model_id:
        # íŠ¹ì • ëª¨ë¸ ë¡œë“œ
        mscn_pilot_model = MscnPilotModel.load_model(load_model_id, "mscn")
    elif not enable_training:
        # í•™ìŠµ ì•ˆí•˜ë©´ ìµœì  ëª¨ë¸ ë¡œë“œ
        registry = ModelRegistry()
        best = registry.get_best_model("mscn", dataset=config.db)
        if best:
            print(f"ğŸ“Š Loading best model for {config.db}")
            mscn_pilot_model = MscnPilotModel.load_model(best["model_id"], "mscn")
        else:
            print("âš ï¸  No trained models, creating new model")
            mscn_pilot_model = MscnPilotModel()
    else:
        # ìƒˆ ëª¨ë¸ ìƒì„±
        mscn_pilot_model = MscnPilotModel()
        
        # í•™ìŠµ ì •ë³´ ì„¤ì •
        hyperparams = {
            "num_epoch": num_epoch,
            "num_training": num_training,
            "num_collection": num_collection
        }
        mscn_pilot_model.set_training_info(
            dataset=config.db,
            hyperparams=hyperparams
        )
    
    # ... ë‚˜ë¨¸ì§€ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ì½”ë“œ ...
    
    return scheduler, mscn_pilot_model  # ëª¨ë¸ë„ ë°˜í™˜
```

---

## ğŸ§ª unified_test.py í†µí•©

```python
# test_example_algorithms/unified_test.py (ìˆ˜ì •)

def run_single_test(config, algo_name, dataset_name, algo_params=None):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ + ìë™ ë©”íƒ€ë°ì´í„° ì €ì¥"""
    
    # ... ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ ...
    
    # í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„
    if algo_name != "baseline":
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance = {
            "total_time": elapsed,
            "average_time": elapsed / len(test_sqls),
            "median_time": ...,  # ê³„ì‚° ê°€ëŠ¥í•˜ë©´
            "p95_time": ...,
        }
        
        # ëª¨ë¸ì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        if hasattr(scheduler_or_interactor, 'pilot_model'):
            model = scheduler_or_interactor.pilot_model
            model.add_test_result(
                dataset=dataset_name,
                num_queries=len(test_sqls),
                performance=performance
            )
            
            # ëª¨ë¸ ì €ì¥ (í…ŒìŠ¤íŠ¸ ê²°ê³¼ í¬í•¨)
            model.save_model()
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
            registry = ModelRegistry()
            registry.register_model(model.metadata)
    
    return result
```

---

## ğŸ› ï¸ CLI ë„êµ¬

```python
# scripts/model_manager.py

import argparse
from algorithm_examples.model_registry import ModelRegistry

def main():
    parser = argparse.ArgumentParser(description='Model Management Tool')
    subparsers = parser.add_subparsers(dest='command')
    
    # list ëª…ë ¹
    list_parser = subparsers.add_parser('list', help='List models')
    list_parser.add_argument('--algo', help='Algorithm filter')
    list_parser.add_argument('--dataset', help='Dataset filter')
    list_parser.add_argument('--tags', nargs='+', help='Tag filter')
    
    # best ëª…ë ¹
    best_parser = subparsers.add_parser('best', help='Find best model')
    best_parser.add_argument('--algo', required=True)
    best_parser.add_argument('--dataset', help='Test dataset')
    
    # compare ëª…ë ¹
    compare_parser = subparsers.add_parser('compare', help='Compare models')
    compare_parser.add_argument('models', nargs='+', help='Model IDs')
    compare_parser.add_argument('--dataset', help='Test dataset filter')
    
    # cleanup ëª…ë ¹
    cleanup_parser = subparsers.add_parser('cleanup', help='Clean old models')
    cleanup_parser.add_argument('--algo', required=True)
    cleanup_parser.add_argument('--keep', type=int, default=5, help='Keep top N')
    
    # tag ëª…ë ¹
    tag_parser = subparsers.add_parser('tag', help='Add tags to model')
    tag_parser.add_argument('model_id')
    tag_parser.add_argument('tags', nargs='+')
    
    args = parser.parse_args()
    registry = ModelRegistry()
    
    if args.command == 'list':
        models = registry.list_models(
            algorithm=args.algo,
            dataset=args.dataset,
            tags=args.tags
        )
        
        print(f"\nFound {len(models)} models:\n")
        for model in models:
            print(f"{'='*80}")
            print(f"Model ID: {model['model_id']}")
            print(f"Algorithm: {model['algorithm']}")
            
            if model['training']:
                train = model['training']
                print(f"Training: {train['dataset']} "
                      f"(epoch={train['hyperparams'].get('num_epoch')}, "
                      f"time={train.get('training_time', 0):.1f}s)")
            
            if model['testing']:
                print(f"Tests: {len(model['testing'])}")
                for test in model['testing']:
                    print(f"  - {test['dataset']}: "
                          f"{test['performance'].get('total_time', 0):.2f}s")
            
            if model.get('tags'):
                print(f"Tags: {', '.join(model['tags'])}")
    
    elif args.command == 'best':
        best = registry.get_best_model(args.algo, dataset=args.dataset)
        if best:
            print(f"\nğŸ† Best Model: {best['model_id']}")
            print(f"Training: {best['training']['dataset']}")
            print(f"Performance:")
            for test in best['testing']:
                if not args.dataset or test['dataset'] == args.dataset:
                    print(f"  {test['dataset']}: "
                          f"{test['performance']['total_time']:.2f}s")
        else:
            print("âŒ No models found")
    
    elif args.command == 'compare':
        registry.compare_models(args.models, test_dataset=args.dataset)
    
    elif args.command == 'cleanup':
        deleted = registry.cleanup_old_models(args.algo, keep_top_n=args.keep)
        print(f"Deleted {len(deleted)} models")
    
    elif args.command == 'tag':
        registry.tag_model(args.model_id, *args.tags)

if __name__ == '__main__':
    main()
```

---

## ğŸ“ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹¤í—˜ì  í•™ìŠµ

```bash
# 1. production ë°ì´í„°ë¡œ í•™ìŠµ (epoch=100)
python unified_test.py --algo mscn --db production --epochs 100
# â†’ ìë™ ì €ì¥: mscn_20241019_103000

# 2. ê²°ê³¼ ì•ˆì¢‹ìœ¼ë©´ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì‹œë„
python unified_test.py --algo mscn --db production --epochs 200
# â†’ ìë™ ì €ì¥: mscn_20241019_110000

# 3. ëª¨ë¸ ë¹„êµ
python scripts/model_manager.py compare \
    mscn_20241019_103000 mscn_20241019_110000

# 4. ì•ˆì¢‹ì€ ëª¨ë¸ ì •ë¦¬
python scripts/model_manager.py cleanup --algo mscn --keep 3
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Train/Test ë¶„ë¦¬

```bash
# 1. ì˜¤ì „: production ë°ì´í„°ë¡œ í•™ìŠµ
python unified_test.py --algo mscn --db production --epochs 100
# â†’ ì €ì¥: mscn_20241019_100000

# 2. ì˜¤í›„: ê·¸ ëª¨ë¸ë¡œ stats_tiny í…ŒìŠ¤íŠ¸ (í•™ìŠµ ì•ˆí•¨)
python unified_test.py \
    --algo mscn \
    --db stats_tiny \
    --no-training \
    --load-model mscn_20241019_100000
# â†’ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: testing ë°°ì—´ì— stats_tiny ê²°ê³¼ ì¶”ê°€

# 3. ì €ë…: ê°™ì€ ëª¨ë¸ë¡œ imdb í…ŒìŠ¤íŠ¸
python unified_test.py \
    --algo mscn \
    --db imdb \
    --no-training \
    --load-model mscn_20241019_100000
# â†’ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: testing ë°°ì—´ì— imdb ê²°ê³¼ ì¶”ê°€

# 4. ê²°ê³¼ í™•ì¸
python scripts/model_manager.py list --algo mscn
# Model: mscn_20241019_100000
#   Training: production (epoch=100)
#   Tests:
#     - production: 45.2s
#     - stats_tiny: 35.1s
#     - imdb: 120.5s
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœì  ëª¨ë¸ ì°¾ê¸°

```bash
# 1. ì—¬ëŸ¬ ë²ˆ ì‹¤í—˜
python unified_test.py --algo mscn --db production --epochs 50
python unified_test.py --algo mscn --db production --epochs 100
python unified_test.py --algo mscn --db production --epochs 200

# 2. ìµœì  ëª¨ë¸ ì¡°íšŒ
python scripts/model_manager.py best --algo mscn --dataset production
# ğŸ† Best Model: mscn_20241019_110000
#   Performance: 42.5s (epoch=100)

# 3. ìµœì  ëª¨ë¸ì— íƒœê·¸
python scripts/model_manager.py tag mscn_20241019_110000 best production

# 4. ë‹¤ìŒì— ìë™ìœ¼ë¡œ ìµœì  ëª¨ë¸ ë¡œë“œ
python unified_test.py --algo mscn --db production --no-training
# â†’ ìë™ìœ¼ë¡œ mscn_20241019_110000 ë¡œë“œ
```

---

## âœ… ì¥ì  ìš”ì•½

| ê¸°ëŠ¥ | íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì´ë¦„ | íƒ€ì„ìŠ¤íƒ¬í”„ + ë©”íƒ€ë°ì´í„° |
|------|------------------|----------------------|
| ì´ë¦„ ì¶©ëŒ | âŒ (ë°ì´í„°ì…‹ ë‹¤ë¥´ë©´ ì¶©ëŒ) | âœ… ì—†ìŒ |
| ì„ì‹œ ì €ì¥ | âŒ (ì´ë¦„ ì„¤ê³„ í•„ìš”) | âœ… ë¶€ë‹´ ì—†ìŒ |
| Train/Test ë¶„ë¦¬ | âŒ | âœ… ì™„ë²½ ì§€ì› |
| ì—¬ëŸ¬ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ | âŒ | âœ… ëˆ„ì  ê°€ëŠ¥ |
| ìµœì  ëª¨ë¸ ì°¾ê¸° | âš ï¸ (ìˆ˜ë™) | âœ… ìë™ |
| ëª¨ë¸ ì •ë¦¬ | ì–´ë ¤ì›€ | âœ… ì‰¬ì›€ |
| ì‹¤í—˜ ì¶”ì  | ì œí•œì  | âœ… ì™„ë²½ |

---

## ğŸ¯ ê²°ë¡ 

ë‹¹ì‹ ì˜ í†µì°°ì´ ì •í™•í•©ë‹ˆë‹¤:

1. **íƒ€ì„ìŠ¤íƒ¬í”„**: ì´ë¦„ ì¶©ëŒ ì—†ì´ "ì„ì‹œ ì €ì¥" ëŠë‚Œìœ¼ë¡œ ë¶€ë‹´ ì—†ì´ ì‹¤í—˜
2. **í’ë¶€í•œ ë©”íƒ€ë°ì´í„°**: ëª¨ë“  ë³€ìˆ˜ (íŒŒë¼ë¯¸í„°, ë°ì´í„°ì…‹, ì„±ëŠ¥) ê¸°ë¡
3. **Train/Test ë¶„ë¦¬**: í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
4. **ë ˆì§€ìŠ¤íŠ¸ë¦¬**: ìµœì  ëª¨ë¸ ì°¾ê¸°, ë¹„êµ, ì •ë¦¬ ìë™í™”

ì´ ë°©ì‹ì´ ì‹¤í—˜ ë‹¨ê³„ì—ì„œ í›¨ì”¬ ì‹¤ìš©ì ì…ë‹ˆë‹¤! ğŸ‰

