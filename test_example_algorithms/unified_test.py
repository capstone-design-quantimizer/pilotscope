#!/usr/bin/env python3
"""
í†µí•© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ - ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ì¡°í•©í•˜ì—¬ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    # ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ ì¡°í•© í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn lero baseline --db stats_tiny production --compare

    # JSON config íŒŒì¼ë¡œ ì‹¤í–‰
    python unified_test.py --config test_configs/production_experiment.json

    # íŠ¹ì • ì¡°í•©ë§Œ í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn --db production --epochs 100 --training-size 500
"""

import sys
sys.path.append("../")

# Force reload of PilotScheduler module to avoid bytecode cache issues
if 'pilotscope.PilotScheduler' in sys.modules:
    del sys.modules['pilotscope.PilotScheduler']
if 'pilotscope.DBController.BaseDBController' in sys.modules:
    del sys.modules['pilotscope.DBController.BaseDBController']

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Optional

from pilotscope.Common.Util import pilotscope_exit
from pilotscope.Common.TimeStatistic import TimeStatistic
from pilotscope.PilotConfig import PilotConfig, PostgreSQLConfig

from algorithm_examples.utils import load_test_sql

# Algorithm Registry
from algorithm_examples.Baseline.BaselinePresetScheduler import get_baseline_preset_scheduler
from algorithm_examples.Mscn.MscnPresetScheduler import get_mscn_preset_scheduler
from algorithm_examples.Lero.LeroPresetScheduler import get_lero_preset_scheduler
from algorithm_examples.KnobTuning.KnobPresetScheduler import get_knob_preset_scheduler
from algorithm_examples.Index.IndexPresetScheduler import get_index_preset_scheduler


# ============================================================================
# Algorithm Registry
# ============================================================================

ALGORITHM_REGISTRY = {
    "baseline": {
        "name": "Baseline (No AI)",
        "factory": get_baseline_preset_scheduler,
        "default_params": {}
    },
    "mscn": {
        "name": "MSCN (Cardinality Estimation)",
        "factory": get_mscn_preset_scheduler,
        "default_params": {
            "enable_collection": False,
            "enable_training": True,
            "num_collection": -1,  # ê° SQLë¥¼ ì§ì ‘ ìˆ˜í–‰í•´ (SQL, Cardinality) GTë¥¼ ë¨¼ì € êµ¬í•¨
            "num_training": -1, # collection ì¤‘ ì¼ë¶€ë¥¼ í™œìš©
            "num_epoch": 100
        }
    },
    "lero": {
        "name": "Lero (Learned Optimizer)",
        "factory": get_lero_preset_scheduler,
        "default_params": {
            "enable_collection": False,
            "enable_training": True,
            "num_collection": 100,  # ê° SQLë¥¼ ì—¬ëŸ¬ cardinalityë³„ë¡œ ì§ì ‘ ìˆ˜í–‰í•´ (SQL, Plans) GTë¥¼ ë¨¼ì € êµ¬í•¨
            "num_training": 500,  # Plan ê°œìˆ˜ (training SQL ë³´ë‹¤ ë§ìŒ)
            "num_epoch": 100
        }
    },
    "knob": {
        "name": "KnobTuning (Configuration Optimization)",
        "factory": get_knob_preset_scheduler,
        "default_params": {}
    },
    "index": {
        "name": "Index Selection (Extend Algorithm)",
        "factory": get_index_preset_scheduler,
        "default_params": {}
    }
}


# ============================================================================
# Test Execution
# ============================================================================

def run_single_test(config: PilotConfig, algo_name: str, db_name: str,
                   workload_name: str = None, algo_params: Dict = None, use_mlflow: bool = True) -> Dict:
    """
    ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ë² ì´ìŠ¤ + ì›Œí¬ë¡œë“œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algo_name: ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ ('mscn', 'lero', 'baseline' ë“±)
        db_name: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ('stats_tiny', 'imdb' ë“±) ë˜ëŠ” ë°ì´í„°ì…‹ ì‹ë³„ì ('stock_strategy_value_investing')
        workload_name: ì›Œí¬ë¡œë“œ ì´ë¦„ (Noneì´ë©´ db_nameê³¼ ë™ì¼, 'custom'ì´ë©´ '{db_name}_custom')
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        use_mlflow: MLflow ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

    Returns:
        Dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì •ë³´
    """
    # Determine actual workload identifier for loading queries
    if workload_name is None or workload_name == "default":
        dataset_name = db_name  # Use default workload
    else:
        dataset_name = f"{db_name}_{workload_name}"  # e.g., "stats_tiny_custom"

    # Set actual PostgreSQL database name
    # For StockStrategy datasets, all workloads share the same 'stock_strategy' DB
    if dataset_name.startswith("stock_strategy"):
        config.db = "stock_strategy"
        actual_db_display = "stock_strategy"
    else:
        config.db = dataset_name
        actual_db_display = dataset_name

    print("\n" + "=" * 60)
    print(f"Testing: {algo_name.upper()} on {dataset_name}")
    if actual_db_display != dataset_name:
        print(f"  Dataset: {dataset_name}")
        print(f"  Actual DB: {actual_db_display}")
    print("=" * 60)

    # Knob Tuningì˜ ê²½ìš° deep control í™œì„±í™” (DB ì¬ì‹œì‘ ê¶Œí•œ í•„ìš”)
    if algo_name == "knob":
        from algorithm_examples.ExampleConfig import example_pg_bin, example_pgdata
        config.enable_deep_control_local(example_pg_bin, example_pgdata)
        print(f"ğŸ”§ Knob Tuning deep control í™œì„±í™”")
        print(f"   PostgreSQL bin: {example_pg_bin}")
        print(f"   PostgreSQL data: {example_pgdata}")

    # Leroì˜ ê²½ìš° íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë¦¼ (LeroëŠ” ì¿¼ë¦¬ë‹¹ ì—¬ëŸ¬ ë²ˆ DB í˜¸ì¶œ)
    if algo_name == "lero":
        original_timeout = config.once_request_timeout
        config.once_request_timeout = 900  # 15ë¶„ìœ¼ë¡œ ì¦ê°€
        print(f"â±ï¸  Lero íƒ€ì„ì•„ì›ƒ ì„¤ì •: {original_timeout}ì´ˆ â†’ {config.once_request_timeout}ì´ˆ")
        print(f"   (LeroëŠ” ì¿¼ë¦¬ë‹¹ ì—¬ëŸ¬ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")

    # TimeStatistic ì´ˆê¸°í™”
    TimeStatistic.clear()

    # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if algo_name not in ALGORITHM_REGISTRY:
        raise ValueError(f"Unknown algorithm: {algo_name}")

    algo_info = ALGORITHM_REGISTRY[algo_name]

    # íŒŒë¼ë¯¸í„° ë³‘í•© (default + user provided)
    params = algo_info.get("default_params", {}).copy()
    if algo_params:
        params.update(algo_params)

    # Add use_mlflow parameter and dataset info
    params['use_mlflow'] = use_mlflow
    params['experiment_name'] = f"{algo_name}_{dataset_name}"  # Full dataset name including workload
    params['dataset_name'] = dataset_name  # For MLflow logging

    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (MLflow ì§€ì› ì•Œê³ ë¦¬ì¦˜ì€ (scheduler, tracker) íŠœí”Œ ë°˜í™˜)
    factory = algo_info["factory"]
    result = factory(config, **params)

    # Handle return value (tuple for MLflow-enabled algorithms, scheduler only for others)
    if isinstance(result, tuple):
        scheduler, mlflow_tracker = result
    else:
        scheduler = result
        mlflow_tracker = None

    # Backup initial DB state for all algorithms (enables safe cleanup later)
    # Even if algorithm doesn't modify config/indexes, backup is harmless
    try:
        if hasattr(scheduler.db_controller, 'backup_config'):
            scheduler.db_controller.backup_config()
    except Exception:
        pass  # Deep control not enabled, skip

    # # ì¤‘ìš”: schedulerê°€ ì–´ëŠ íŒŒì¼ì—ì„œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    # import inspect
    # print(f"\nğŸ” Scheduler module file: {inspect.getfile(scheduler.__class__)}")
    # print(f"   Scheduler execute method file: {inspect.getfile(scheduler.execute)}")
    
    # # execute ë©”ì„œë“œì˜ ì†ŒìŠ¤ ì½”ë“œ ì²« ì¤„ í™•ì¸
    # source_lines = inspect.getsourcelines(scheduler.execute)[0][:20]
    # print(f"   First 5 lines of execute():")
    # for line in source_lines:
    #     print(f"     {line.rstrip()}")
    
    # Check if we should skip testing (when only collecting data)
    if algo_params and not algo_params.get('enable_training', True):
        print(f"\nâ­ï¸  Skipping test phase (--no-training specified)")
        print(f"   âœ… Data collection completed!")
        print(f"\nğŸ’¡ Next step: Train with collected data:")
        print(f"   python unified_test.py --algo {algo_name} --db {db_name}", end="")
        if workload_name:
            print(f" --workload {workload_name}", end="")
        print(f" \\\n       --no-collection --training-size 500 --epochs 100")

        # End MLflow run if exists
        run_id = None
        if mlflow_tracker:
            run_id = mlflow_tracker.run_id
            mlflow_tracker.end_run(status="FINISHED")
            print(f"\n   âœ“ MLflow run completed: {run_id}")

        return {
            "algorithm": algo_name,
            "database": db_name,
            "workload": workload_name or "default",
            "dataset": dataset_name,
            "elapsed_time": 0,
            "params": algo_params,
            "mlflow_run_id": run_id,
            "skipped": "no_training"
        }

    # í…ŒìŠ¤íŠ¸ SQL ë¡œë“œ
    print(f"\nğŸ“‚ Loading test queries from {dataset_name}...")
    try:
        test_sqls = load_test_sql(dataset_name)
        print(f"   Loaded {len(test_sqls)} queries")
    except Exception as e:
        print(f"âŒ Failed to load test queries: {e}")
        return None

    # ì¿¼ë¦¬ ì‹¤í–‰
    print(f"\nğŸš€ Running {len(test_sqls)} queries...")
    start_time = time.time()
    
    # ì‹¤ì œ DB ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¶”ì  (AI ëª¨ë¸ ì¶”ë¡  ì‹œê°„ ì œì™¸)
    total_db_execution_time = 0.0
    
    try:
        for i, sql in enumerate(test_sqls):
            if i % 10 == 0:
                print(f"   Progress: {i}/{len(test_sqls)} queries")
            
            try:
                # Python ë ˆë²¨ ì‹œê°„ ì¸¡ì • (ì „ì²´ ì˜¤ë²„í—¤ë“œ í¬í•¨)
                TimeStatistic.start(algo_name.capitalize())
                result = scheduler.execute(sql)
                TimeStatistic.end(algo_name.capitalize())
                
                # ì‹¤ì œ DB ì‹¤í–‰ ì‹œê°„ ì¶”ì¶œ (pull_execution_time=Trueë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°)
                if hasattr(scheduler, 'last_execution_data'):
                    exec_data = scheduler.last_execution_data
                    if exec_data is not None and hasattr(exec_data, 'execution_time') and exec_data.execution_time is not None:
                        # execution_timeì´ millisecondsì¸ ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                        exec_time = float(exec_data.execution_time)
                        # DBì— ë”°ë¼ millisecondsë¡œ ì €ì¥ë  ìˆ˜ ìˆìŒ (PostgreSQLì€ ë³´í†µ milliseconds)
                        if exec_time > 1000:  # 1000ms ì´ìƒì´ë©´ ë°€ë¦¬ì´ˆë¡œ ê°„ì£¼
                            exec_time = exec_time / 1000.0
                        total_db_execution_time += exec_time
            except Exception as e:
                print(f"âš ï¸  Query {i} failed: {e}")
    finally:
        # ìŠ¤ì¼€ì¤„ëŸ¬/ì¸í„°ë™í„° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        print(f"\nğŸ§¹ Cleaning up resources for {algo_name}...")
        if hasattr(scheduler, 'data_interactor'):
            try:
                # Reset data_interactor to remove all registered anchors
                scheduler.data_interactor.reset()
                print(f"   âœ“ data_interactor reset")
            except Exception as cleanup_err:
                print(f"   âš ï¸  Warning: Failed to reset data_interactor: {cleanup_err}")
        if hasattr(scheduler, 'db_controller'):
            try:
                # Cleanup DB controller connections
                scheduler.db_controller.cleanup()
                print(f"   âœ“ db_controller cleanup")
            except Exception as cleanup_err:
                print(f"   âš ï¸  Warning: Failed to cleanup db_controller: {cleanup_err}")
    
    end_time = time.time()
    elapsed = end_time - start_time

    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ Saving results...")
    name_2_value = TimeStatistic.get_sum_data()

    # ìˆœìˆ˜ DB ì‹¤í–‰ ì‹œê°„ ì •ë³´ ì¶œë ¥
    print(f"\nâ±ï¸  Timing breakdown:")
    print(f"   Total wall time: {elapsed:.3f}s (Python ë ˆë²¨, ëª¨ë“  ì˜¤ë²„í—¤ë“œ í¬í•¨)")
    if total_db_execution_time > 0:
        print(f"   DB execution time: {total_db_execution_time:.3f}s (ìˆœìˆ˜ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„)")
        print(f"   Overhead: {elapsed - total_db_execution_time:.3f}s (AI ì¶”ë¡  + ë°ì´í„° ìˆ˜ì§‘)")
    else:
        print(f"   DB execution time: N/A (execution_time not collected)")

    # Prepare metrics
    test_metrics = {
        "total_time": elapsed,
        "average_time": elapsed / len(test_sqls) if test_sqls else 0,
        "query_count": len(test_sqls),
        "db_execution_time": total_db_execution_time,
        "overhead_time": elapsed - total_db_execution_time if total_db_execution_time > 0 else 0
    }

    # Log to MLflow if tracker exists
    if mlflow_tracker:
        print(f"ğŸ“Š Logging test results to MLflow...")
        mlflow_tracker.log_test_results(test_metrics, test_dataset=dataset_name, num_test_queries=len(test_sqls))

        # Update model with test results (save_model() will handle MLflow upload)
        if algo_name != "baseline" and hasattr(scheduler, 'pilot_model'):
            model = scheduler.pilot_model
            if hasattr(model, 'add_test_result'):
                performance = {
                    "total_time": elapsed,
                    "average_time": elapsed / len(test_sqls) if test_sqls else 0,
                    "num_queries": len(test_sqls),
                    "db_execution_time": total_db_execution_time
                }
                model.add_test_result(dataset_name, len(test_sqls), performance)
                # save_model() now automatically uploads to MLflow via mlflow_tracker
                model.save_model()
                print(f"   âœ“ Model updated with test results")

        # End MLflow run (save run_id before it's cleared)
        run_id = mlflow_tracker.run_id
        mlflow_tracker.end_run(status="FINISHED")
        print(f"   âœ“ MLflow run completed: {run_id}")

    print(f"\nâœ… Test completed in {elapsed:.2f}s")

    # Cleanup: Always restore DB to initial state
    print(f"\nğŸ§¹ Restoring database to initial state...")
    try:
        # Drop all user-created indexes (safe even if none were created)
        scheduler.db_controller.drop_all_indexes()
        print("   âœ“ Indexes cleaned")
    except Exception as e:
        # Expected for algorithms without deep control or if no indexes exist
        pass

    try:
        # Recover original config and restart (safe because we backed up earlier)
        if hasattr(scheduler.db_controller, 'recover_config'):
            scheduler.db_controller.recover_config()
            scheduler.db_controller.restart()
            print("   âœ“ PostgreSQL configuration restored")
    except Exception as e:
        # Expected if deep control not enabled
        pass

    return {
        "algorithm": algo_name,
        "database": db_name,
        "workload": workload_name or "default",
        "dataset": dataset_name,  # For backward compatibility
        "elapsed_time": elapsed,
        "params": params,
        "mlflow_run_id": mlflow_tracker.run_id if mlflow_tracker else None
    }


def run_multiple_tests(config: PilotConfig, algorithms: List[str],
                      databases: List[str], workload: str = None, algo_params: Dict = None) -> List[Dict]:
    """
    ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ë² ì´ìŠ¤ + ì›Œí¬ë¡œë“œ ì¡°í•©ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

    Note: ì´ í•¨ìˆ˜ëŠ” ì´ì œ deprecatedë˜ì—ˆìŠµë‹ˆë‹¤. ê°„ì†Œí™”ë¥¼ ìœ„í•´ run_single_testë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algorithms: í…ŒìŠ¤íŠ¸í•  ì•Œê³ ë¦¬ì¦˜ ë¦¬ìŠ¤íŠ¸
        databases: í…ŒìŠ¤íŠ¸í•  ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        workload: ì›Œí¬ë¡œë“œ ì´ë¦„ (Noneì´ë©´ default, 'custom' ë“±)
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° (algo_nameì„ í‚¤ë¡œ í•˜ëŠ” dict)

    Returns:
        List[Dict]: ê° í…ŒìŠ¤íŠ¸ì˜ ê²°ê³¼ ì •ë³´
    """
    results = []

    for db_name in databases:
        config.db = db_name  # Set actual database name

        for algo in algorithms:
            # ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            params = algo_params.get(algo, {}) if algo_params else {}

            try:
                result = run_single_test(config, algo, db_name, workload, params)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"\nâŒ Test failed for {algo} on {db_name} (workload: {workload}): {e}")
                import traceback
                traceback.print_exc()
            finally:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë” ì² ì €í•˜ê²Œ
                print(f"\nğŸ§¹ Cleaning up resources for {algo} on {db_name}...")

                try:
                    pilotscope_exit()
                except Exception as e:
                    print(f"âš ï¸  Error during pilotscope_exit: {e}")

                # ê°•ì œë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                import gc
                gc.collect()

                # ì§§ì€ ëŒ€ê¸° ì‹œê°„ (DB ì—°ê²° ì™„ì „íˆ ë‹«íˆë„ë¡)
                import time
                time.sleep(2)

    return results


# ============================================================================
# JSON Config File Support
# ============================================================================

def load_config_file(config_file: str) -> Dict:
    """JSON config íŒŒì¼ ë¡œë“œ"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_from_config_file(config_file: str):
    """
    JSON config íŒŒì¼ì—ì„œ ì‹¤í—˜ ì„¤ì •ì„ ì½ì–´ ì‹¤í–‰
    
    Config íŒŒì¼ í˜•ì‹:
    {
      "db_config": {...},
      "experiments": [
        {
          "name": "exp1",
          "algorithm": "mscn",
          "dataset": "stats_tiny",
          "params": {...}
        }
      ],
      "comparison": {...}
    }
    """
    print(f"\nğŸ“„ Loading config from: {config_file}")
    config_data = load_config_file(config_file)
    
    # DB Config ìƒì„±
    db_config_data = config_data.get("db_config", {})
    config = PostgreSQLConfig(**db_config_data)
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiments = config_data.get("experiments", [])
    results = []
    
    for exp in experiments:
        exp_name = exp.get("name", "unnamed")
        algo = exp.get("algorithm")
        db_name = exp.get("database", exp.get("dataset"))  # Support both 'database' and legacy 'dataset'
        workload = exp.get("workload", None)
        params = exp.get("params", {})

        print(f"\nğŸ§ª Running experiment: {exp_name}")
        config.db = db_name

        result = run_single_test(config, algo, db_name, workload, params)
        if result:
            result["experiment_name"] = exp_name
            results.append(result)
        
        pilotscope_exit()
    
    return results


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Unified test framework for multiple algorithms and datasets',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test single algorithm on single database
  python unified_test.py --algo mscn --db stats_tiny

  # Test with custom workload on stats_tiny database
  python unified_test.py --algo mscn --db stats_tiny --workload custom --epochs 100

  # Test with custom parameters
  python unified_test.py --algo mscn --db stats_tiny --epochs 50 --training-size 500

  # Use JSON config file
  python unified_test.py --config test_configs/production_experiment.json
        """
    )
    
    # Mode selection
    parser.add_argument('--config', help='JSON config file path')
    
    # Algorithm & Dataset selection
    parser.add_argument('--algo',
                       choices=list(ALGORITHM_REGISTRY.keys()),
                       help='Algorithm to test')
    parser.add_argument('--db',
                       help='Database to test (e.g., stats_tiny, imdb, stock_strategy_value_investing)')
    parser.add_argument('--workload',
                       help='Workload to use (default: same as db, custom: use custom workload)')
    
    # Algorithm parameters
    parser.add_argument('--epochs', type=int, help='Number of training epochs')
    parser.add_argument('--training-size', type=int, 
                       help='Number of queries to use for training (-1 for all)')
    parser.add_argument('--collection-size', type=int,
                       help='Number of queries to collect for training (-1 for all)')
    parser.add_argument('--no-collection', action='store_true',
                       help='Disable data collection (use existing data)')
    parser.add_argument('--no-training', action='store_true',
                       help='Disable model training (use existing model)')

    # DB Config
    parser.add_argument('--db-host', help='Database host')
    parser.add_argument('--db-port', help='Database port')
    parser.add_argument('--db-user', help='Database user')
    parser.add_argument('--db-pwd', help='Database password')
    parser.add_argument('--timeout', type=int, 
                       help='Request timeout in seconds (default: 600, recommend 900+ for Lero)')
    
    args = parser.parse_args()
    
    # JSON Config íŒŒì¼ ëª¨ë“œ
    if args.config:
        run_from_config_file(args.config)
        return
    
    # CLI ëª¨ë“œ
    if not args.algo or not args.db:
        parser.print_help()
        print("\nâŒ Error: --algo and --db are required (or use --config)")
        return
    
    # DB Config ìƒì„±
    config = PostgreSQLConfig()
    if args.db_host:
        config.db_host = args.db_host
    if args.db_port:
        config.db_port = args.db_port
    if args.db_user:
        config.db_user = args.db_user
    if args.db_pwd:
        config.db_user_pwd = args.db_pwd
    if args.timeout:
        config.once_request_timeout = args.timeout
        config.sql_execution_timeout = args.timeout
        print(f"â±ï¸  íƒ€ì„ì•„ì›ƒ ì„¤ì •: {args.timeout}ì´ˆ")
    
    # ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {}
    if args.epochs is not None:
        params['num_epoch'] = args.epochs
    if args.training_size is not None:
        params['num_training'] = args.training_size
    if args.collection_size is not None:
        params['num_collection'] = args.collection_size
        # Automatically enable collection when collection size is specified
        params['enable_collection'] = True
    if args.no_collection:
        params['enable_collection'] = False
    if args.no_training:
        params['enable_training'] = False

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        result = run_single_test(config, args.algo, args.db, args.workload, params)

        if result:
            print("\n" + "=" * 60)
            print("âœ¨ Test completed!")
            print("=" * 60)
        else:
            print("\n" + "=" * 60)
            print("âš ï¸  Test completed with issues")
            print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        pilotscope_exit()


if __name__ == '__main__':
    main()

