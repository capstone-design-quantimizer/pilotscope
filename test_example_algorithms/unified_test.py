#!/usr/bin/env python3
"""
í†µí•© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ - ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ì¡°í•©í•˜ì—¬ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    # ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ ì¡°í•© í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn lero baseline --db stats_tiny production --compare

    # JSON config íŒŒì¼ë¡œ ì‹¤í–‰
    python unified_test.py --config test_configs/production_experiment.json

    # íŠ¹ì • ì¡°í•©ë§Œ í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn --db production --epochs 100 --training-size 500
"""

import sys
sys.path.append("../")

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Optional

from pilotscope.Common.Util import pilotscope_exit
from pilotscope.Common.TimeStatistic import TimeStatistic
from pilotscope.Common.Drawer import Drawer
from pilotscope.PilotConfig import PilotConfig, PostgreSQLConfig
from pilotscope.DBInteractor.PilotDataInteractor import PilotDataInteractor

from algorithm_examples.utils import load_test_sql, save_test_result, compare_algorithms
from algorithm_examples.ExampleConfig import get_time_statistic_img_path

# Algorithm Registry
from algorithm_examples.Baseline.BaselinePresetScheduler import get_baseline_preset_scheduler
from algorithm_examples.Mscn.MscnPresetScheduler import get_mscn_preset_scheduler
from algorithm_examples.Lero.LeroPresetScheduler import get_lero_preset_scheduler
from algorithm_examples.KnobTuning.KnobPresetScheduler import get_knob_preset_scheduler


# ============================================================================
# Algorithm Registry
# ============================================================================

ALGORITHM_REGISTRY = {
    "baseline": {
        "name": "Baseline (No AI)",
        "factory": get_baseline_preset_scheduler,
        "default_params": {}
    },
    "mscn": {
        "name": "MSCN (Cardinality Estimation)",
        "factory": get_mscn_preset_scheduler,
        "default_params": {
            "enable_collection": True,
            "enable_training": True,
            "num_collection": -1,
            "num_training": -1,
            "num_epoch": 100
        }
    },
    "lero": {
        "name": "Lero (Learned Optimizer)",
        "factory": get_lero_preset_scheduler,
        "default_params": {
            "enable_collection": True,
            "enable_training": True,
            "num_collection": -1,
            "num_training": -1,
            "num_epoch": 100
        }
    },
    "knob": {
        "name": "KnobTuning (Configuration Optimization)",
        "factory": get_knob_preset_scheduler,
        "default_params": {}
    }
}


# ============================================================================
# Test Execution
# ============================================================================

def run_single_test(config: PilotConfig, algo_name: str, dataset_name: str, 
                   algo_params: Dict = None) -> Dict:
    """
    ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ì…‹ ì¡°í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algo_name: ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ ('mscn', 'lero', 'baseline' ë“±)
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ ('stats_tiny', 'production' ë“±)
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    
    Returns:
        Dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì •ë³´
    """
    print("\n" + "=" * 60)
    print(f"Testing: {algo_name.upper()} on {dataset_name}")
    print("=" * 60)
    
    # TimeStatistic ì´ˆê¸°í™”
    TimeStatistic.reset()
    
    # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if algo_name not in ALGORITHM_REGISTRY:
        raise ValueError(f"Unknown algorithm: {algo_name}")
    
    algo_info = ALGORITHM_REGISTRY[algo_name]
    
    # íŒŒë¼ë¯¸í„° ë³‘í•© (default + user provided)
    params = algo_info.get("default_params", {}).copy()
    if algo_params:
        params.update(algo_params)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë™ì¼í•œ íŒ¨í„´)
    factory = algo_info["factory"]
    scheduler_or_interactor = factory(config, **params)
    
    # í…ŒìŠ¤íŠ¸ SQL ë¡œë“œ
    print(f"\nğŸ“‚ Loading test queries from {dataset_name}...")
    try:
        test_sqls = load_test_sql(dataset_name)
        print(f"   Loaded {len(test_sqls)} queries")
    except Exception as e:
        print(f"âŒ Failed to load test queries: {e}")
        return None
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    print(f"\nğŸš€ Running {len(test_sqls)} queries...")
    start_time = time.time()
    
    for i, sql in enumerate(test_sqls):
        if i % 10 == 0:
            print(f"   Progress: {i}/{len(test_sqls)} queries")
        
        try:
            TimeStatistic.start(algo_name.capitalize())
            scheduler_or_interactor.execute(sql)
            TimeStatistic.end(algo_name.capitalize())
        except Exception as e:
            print(f"âš ï¸  Query {i} failed: {e}")
    
    end_time = time.time()
    elapsed = end_time - start_time
    
    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ Saving results...")
    name_2_value = TimeStatistic.get_sum_data()
    
    result_file = save_test_result(algo_name, dataset_name, extra_info={
        "params": params,
        "num_queries": len(test_sqls),
        "wall_time": elapsed
    })
    
    # ì‹œê°í™”
    img_path = get_time_statistic_img_path(algo_name, dataset_name)
    Drawer.draw_bar(name_2_value, img_path, is_rotation=False)
    
    # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (AI ì•Œê³ ë¦¬ì¦˜ì¸ ê²½ìš°)
    if algo_name != "baseline" and hasattr(scheduler_or_interactor, 'pilot_model'):
        from pilotscope.ModelRegistry import ModelRegistry
        
        model = scheduler_or_interactor.pilot_model
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance = {
            "total_time": elapsed,
            "average_time": elapsed / len(test_sqls) if test_sqls else 0,
            "num_queries": len(test_sqls)
        }
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        model.add_test_result(dataset_name, len(test_sqls), performance)
        
        # ëª¨ë¸ ì €ì¥ (ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸)
        model.save_model()
        
        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
        registry = ModelRegistry()
        registry.register_model(model.metadata)
        
        print(f"âœ… Model metadata saved: {model.model_id}")
    
    print(f"\nâœ… Test completed in {elapsed:.2f}s")
    print(f"   Result: {result_file}")
    print(f"   Chart:  img/{img_path}.png")
    
    return {
        "algorithm": algo_name,
        "dataset": dataset_name,
        "result_file": str(result_file),
        "elapsed_time": elapsed,
        "params": params
    }


def run_multiple_tests(config: PilotConfig, algorithms: List[str], 
                      datasets: List[str], algo_params: Dict = None) -> List[Dict]:
    """
    ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ì…‹ ì¡°í•©ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    
    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algorithms: í…ŒìŠ¤íŠ¸í•  ì•Œê³ ë¦¬ì¦˜ ë¦¬ìŠ¤íŠ¸
        datasets: í…ŒìŠ¤íŠ¸í•  ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° (algo_nameì„ í‚¤ë¡œ í•˜ëŠ” dict)
    
    Returns:
        List[Dict]: ê° í…ŒìŠ¤íŠ¸ì˜ ê²°ê³¼ ì •ë³´
    """
    results = []
    
    for dataset in datasets:
        config.db = dataset
        
        for algo in algorithms:
            # ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            params = algo_params.get(algo, {}) if algo_params else {}
            
            try:
                result = run_single_test(config, algo, dataset, params)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"\nâŒ Test failed for {algo} on {dataset}: {e}")
            finally:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                try:
                    pilotscope_exit()
                except:
                    pass
    
    return results


# ============================================================================
# Comparison & Report
# ============================================================================

def compare_results(results: List[Dict], output_dir: str = "results"):
    """
    ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¹„êµ
    
    Args:
        results: run_multiple_tests()ì˜ ë°˜í™˜ê°’
        output_dir: ë¹„êµ ì°¨íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    if len(results) < 2:
        print("\nâš ï¸  Need at least 2 results to compare")
        return
    
    print("\n" + "=" * 60)
    print("Comparison Summary")
    print("=" * 60)
    
    # ê²°ê³¼ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    result_files = [r["result_file"] for r in results]
    
    # ë¹„êµ ì‹¤í–‰
    output_path = f"{output_dir}/comparison_all"
    compare_algorithms(result_files, metric='total_time', output_path=output_path)
    
    # ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š Test Results:")
    print("-" * 60)
    for result in sorted(results, key=lambda x: x["elapsed_time"]):
        print(f"  {result['algorithm']:10s} on {result['dataset']:15s}: {result['elapsed_time']:8.2f}s")
    print("-" * 60)


# ============================================================================
# JSON Config File Support
# ============================================================================

def load_config_file(config_file: str) -> Dict:
    """JSON config íŒŒì¼ ë¡œë“œ"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_from_config_file(config_file: str):
    """
    JSON config íŒŒì¼ì—ì„œ ì‹¤í—˜ ì„¤ì •ì„ ì½ì–´ ì‹¤í–‰
    
    Config íŒŒì¼ í˜•ì‹:
    {
      "db_config": {...},
      "experiments": [
        {
          "name": "exp1",
          "algorithm": "mscn",
          "dataset": "stats_tiny",
          "params": {...}
        }
      ],
      "comparison": {...}
    }
    """
    print(f"\nğŸ“„ Loading config from: {config_file}")
    config_data = load_config_file(config_file)
    
    # DB Config ìƒì„±
    db_config_data = config_data.get("db_config", {})
    config = PostgreSQLConfig(**db_config_data)
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiments = config_data.get("experiments", [])
    results = []
    
    for exp in experiments:
        exp_name = exp.get("name", "unnamed")
        algo = exp.get("algorithm")
        dataset = exp.get("dataset")
        params = exp.get("params", {})
        
        print(f"\nğŸ§ª Running experiment: {exp_name}")
        config.db = dataset
        
        result = run_single_test(config, algo, dataset, params)
        if result:
            result["experiment_name"] = exp_name
            results.append(result)
        
        pilotscope_exit()
    
    # ë¹„êµ
    comparison_config = config_data.get("comparison", {})
    if comparison_config.get("enabled", True):
        compare_results(results, 
                       output_dir=comparison_config.get("output_dir", "results"))
    
    return results


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Unified test framework for multiple algorithms and datasets',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test multiple algorithms on multiple datasets
  python unified_test.py --algo mscn lero baseline --db stats_tiny production --compare
  
  # Test with custom parameters
  python unified_test.py --algo mscn --db production --epochs 50 --training-size 500
  
  # Use JSON config file
  python unified_test.py --config test_configs/production_experiment.json
        """
    )
    
    # Mode selection
    parser.add_argument('--config', help='JSON config file path')
    
    # Algorithm & Dataset selection
    parser.add_argument('--algo', nargs='+', 
                       choices=list(ALGORITHM_REGISTRY.keys()),
                       help='Algorithms to test')
    parser.add_argument('--db', nargs='+', 
                       help='Datasets to test (e.g., stats_tiny, imdb, production)')
    
    # Algorithm parameters
    parser.add_argument('--epochs', type=int, help='Number of training epochs')
    parser.add_argument('--training-size', type=int, 
                       help='Number of queries to use for training (-1 for all)')
    parser.add_argument('--collection-size', type=int,
                       help='Number of queries to collect for training (-1 for all)')
    parser.add_argument('--no-collection', action='store_true',
                       help='Disable data collection (use existing data)')
    parser.add_argument('--no-training', action='store_true',
                       help='Disable model training (use existing model)')
    
    # Output options
    parser.add_argument('--compare', action='store_true',
                       help='Compare results after all tests')
    parser.add_argument('--output-dir', default='results',
                       help='Output directory for results (default: results)')
    
    # DB Config
    parser.add_argument('--db-host', help='Database host')
    parser.add_argument('--db-port', help='Database port')
    parser.add_argument('--db-user', help='Database user')
    parser.add_argument('--db-pwd', help='Database password')
    
    args = parser.parse_args()
    
    # JSON Config íŒŒì¼ ëª¨ë“œ
    if args.config:
        run_from_config_file(args.config)
        return
    
    # CLI ëª¨ë“œ
    if not args.algo or not args.db:
        parser.print_help()
        print("\nâŒ Error: --algo and --db are required (or use --config)")
        return
    
    # DB Config ìƒì„±
    config = PostgreSQLConfig()
    if args.db_host:
        config.db_host = args.db_host
    if args.db_port:
        config.db_port = args.db_port
    if args.db_user:
        config.db_user = args.db_user
    if args.db_pwd:
        config.db_user_pwd = args.db_pwd
    
    # ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„° ì„¤ì •
    algo_params = {}
    for algo in args.algo:
        params = {}
        
        if args.epochs is not None:
            params['num_epoch'] = args.epochs
        if args.training_size is not None:
            params['num_training'] = args.training_size
        if args.collection_size is not None:
            params['num_collection'] = args.collection_size
        if args.no_collection:
            params['enable_collection'] = False
        if args.no_training:
            params['enable_training'] = False
        
        algo_params[algo] = params
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        results = run_multiple_tests(config, args.algo, args.db, algo_params)
        
        # ë¹„êµ
        if args.compare and len(results) > 1:
            compare_results(results, args.output_dir)
        
        print("\n" + "=" * 60)
        print("âœ¨ All tests completed!")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        pilotscope_exit()


if __name__ == '__main__':
    main()

