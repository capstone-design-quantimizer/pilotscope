#!/usr/bin/env python3
"""
í†µí•© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ - ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ì¡°í•©í•˜ì—¬ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    # ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„°ì…‹ ì¡°í•© í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn lero baseline --db stats_tiny production --compare

    # JSON config íŒŒì¼ë¡œ ì‹¤í–‰
    python unified_test.py --config test_configs/production_experiment.json

    # íŠ¹ì • ì¡°í•©ë§Œ í…ŒìŠ¤íŠ¸
    python unified_test.py --algo mscn --db production --epochs 100 --training-size 500
"""

import sys
sys.path.append("../")

# Force reload of PilotScheduler module to avoid bytecode cache issues
if 'pilotscope.PilotScheduler' in sys.modules:
    del sys.modules['pilotscope.PilotScheduler']
if 'pilotscope.DBController.BaseDBController' in sys.modules:
    del sys.modules['pilotscope.DBController.BaseDBController']

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Optional

from pilotscope.Common.Util import pilotscope_exit
from pilotscope.Common.TimeStatistic import TimeStatistic
from pilotscope.Common.Drawer import Drawer
from pilotscope.PilotConfig import PilotConfig, PostgreSQLConfig
from pilotscope.DBInteractor.PilotDataInteractor import PilotDataInteractor

from algorithm_examples.utils import load_test_sql, save_test_result, compare_algorithms
from algorithm_examples.ExampleConfig import get_time_statistic_img_path

# Algorithm Registry
from algorithm_examples.Baseline.BaselinePresetScheduler import get_baseline_preset_scheduler
from algorithm_examples.Mscn.MscnPresetScheduler import get_mscn_preset_scheduler
from algorithm_examples.Lero.LeroPresetScheduler import get_lero_preset_scheduler
from algorithm_examples.KnobTuning.KnobPresetScheduler import get_knob_preset_scheduler


# ============================================================================
# Algorithm Registry
# ============================================================================

ALGORITHM_REGISTRY = {
    "baseline": {
        "name": "Baseline (No AI)",
        "factory": get_baseline_preset_scheduler,
        "default_params": {}
    },
    "mscn": {
        "name": "MSCN (Cardinality Estimation)",
        "factory": get_mscn_preset_scheduler,
        "default_params": {
            "enable_collection": True,
            "enable_training": True,
            "num_collection": -1,
            "num_training": -1,
            "num_epoch": 100
        }
    },
    "lero": {
        "name": "Lero (Learned Optimizer)",
        "factory": get_lero_preset_scheduler,
        "default_params": {
            "enable_collection": True,
            "enable_training": True,
            "num_collection": 100,  # ê¸°ë³¸ê°’ì„ 100ìœ¼ë¡œ ì œí•œ (LeroëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
            "num_training": 500,
            "num_epoch": 100
        }
    },
    "knob": {
        "name": "KnobTuning (Configuration Optimization)",
        "factory": get_knob_preset_scheduler,
        "default_params": {}
    }
}


# ============================================================================
# Test Execution
# ============================================================================

def run_single_test(config: PilotConfig, algo_name: str, dataset_name: str, 
                   algo_params: Dict = None) -> Dict:
    """
    ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ì…‹ ì¡°í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algo_name: ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ ('mscn', 'lero', 'baseline' ë“±)
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ ('stats_tiny', 'production' ë“±)
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    
    Returns:
        Dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì •ë³´
    """
    print("\n" + "=" * 60)
    print(f"Testing: {algo_name.upper()} on {dataset_name}")
    print("=" * 60)
    
    # Leroì˜ ê²½ìš° íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë¦¼ (LeroëŠ” ì¿¼ë¦¬ë‹¹ ì—¬ëŸ¬ ë²ˆ DB í˜¸ì¶œ)
    if algo_name == "lero":
        original_timeout = config.once_request_timeout
        config.once_request_timeout = 900  # 15ë¶„ìœ¼ë¡œ ì¦ê°€
        print(f"â±ï¸  Lero íƒ€ì„ì•„ì›ƒ ì„¤ì •: {original_timeout}ì´ˆ â†’ {config.once_request_timeout}ì´ˆ")
        print(f"   (LeroëŠ” ì¿¼ë¦¬ë‹¹ ì—¬ëŸ¬ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")
    
    # TimeStatistic ì´ˆê¸°í™”
    TimeStatistic.clear()
    
    # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if algo_name not in ALGORITHM_REGISTRY:
        raise ValueError(f"Unknown algorithm: {algo_name}")
    
    algo_info = ALGORITHM_REGISTRY[algo_name]
    
    # íŒŒë¼ë¯¸í„° ë³‘í•© (default + user provided)
    params = algo_info.get("default_params", {}).copy()
    if algo_params:
        params.update(algo_params)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë™ì¼í•œ íŒ¨í„´)
    factory = algo_info["factory"]
    scheduler = factory(config, **params)
    
    # # ì¤‘ìš”: schedulerê°€ ì–´ëŠ íŒŒì¼ì—ì„œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    # import inspect
    # print(f"\nğŸ” Scheduler module file: {inspect.getfile(scheduler.__class__)}")
    # print(f"   Scheduler execute method file: {inspect.getfile(scheduler.execute)}")
    
    # # execute ë©”ì„œë“œì˜ ì†ŒìŠ¤ ì½”ë“œ ì²« ì¤„ í™•ì¸
    # source_lines = inspect.getsourcelines(scheduler.execute)[0][:20]
    # print(f"   First 5 lines of execute():")
    # for line in source_lines:
    #     print(f"     {line.rstrip()}")
    
    # í…ŒìŠ¤íŠ¸ SQL ë¡œë“œ
    print(f"\nğŸ“‚ Loading test queries from {dataset_name}...")
    try:
        test_sqls = load_test_sql(dataset_name)
        print(f"   Loaded {len(test_sqls)} queries")
    except Exception as e:
        print(f"âŒ Failed to load test queries: {e}")
        return None
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    print(f"\nğŸš€ Running {len(test_sqls)} queries...")
    start_time = time.time()
    
    # ì‹¤ì œ DB ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¶”ì  (AI ëª¨ë¸ ì¶”ë¡  ì‹œê°„ ì œì™¸)
    total_db_execution_time = 0.0
    
    try:
        for i, sql in enumerate(test_sqls):
            if i % 10 == 0:
                print(f"   Progress: {i}/{len(test_sqls)} queries")
            
            try:
                # Python ë ˆë²¨ ì‹œê°„ ì¸¡ì • (ì „ì²´ ì˜¤ë²„í—¤ë“œ í¬í•¨)
                TimeStatistic.start(algo_name.capitalize())
                result = scheduler.execute(sql)
                TimeStatistic.end(algo_name.capitalize())
                
                # ì‹¤ì œ DB ì‹¤í–‰ ì‹œê°„ ì¶”ì¶œ (pull_execution_time=Trueë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°)
                if hasattr(scheduler, 'last_execution_data'):
                    exec_data = scheduler.last_execution_data
                    if exec_data is not None and hasattr(exec_data, 'execution_time') and exec_data.execution_time is not None:
                        # execution_timeì´ millisecondsì¸ ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                        exec_time = float(exec_data.execution_time)
                        # DBì— ë”°ë¼ millisecondsë¡œ ì €ì¥ë  ìˆ˜ ìˆìŒ (PostgreSQLì€ ë³´í†µ milliseconds)
                        if exec_time > 1000:  # 1000ms ì´ìƒì´ë©´ ë°€ë¦¬ì´ˆë¡œ ê°„ì£¼
                            exec_time = exec_time / 1000.0
                        total_db_execution_time += exec_time
            except Exception as e:
                print(f"âš ï¸  Query {i} failed: {e}")
    finally:
        # ìŠ¤ì¼€ì¤„ëŸ¬/ì¸í„°ë™í„° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        print(f"\nğŸ§¹ Cleaning up resources for {algo_name}...")
        if hasattr(scheduler, 'data_interactor'):
            try:
                # Reset data_interactor to remove all registered anchors
                scheduler.data_interactor.reset()
                print(f"   âœ“ data_interactor reset")
            except Exception as cleanup_err:
                print(f"   âš ï¸  Warning: Failed to reset data_interactor: {cleanup_err}")
        if hasattr(scheduler, 'db_controller'):
            try:
                # Cleanup DB controller connections
                scheduler.db_controller.cleanup()
                print(f"   âœ“ db_controller cleanup")
            except Exception as cleanup_err:
                print(f"   âš ï¸  Warning: Failed to cleanup db_controller: {cleanup_err}")
    
    end_time = time.time()
    elapsed = end_time - start_time
    
    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ Saving results...")
    name_2_value = TimeStatistic.get_sum_data()
    
    # ìˆœìˆ˜ DB ì‹¤í–‰ ì‹œê°„ ì •ë³´ ì¶œë ¥
    print(f"\nâ±ï¸  Timing breakdown:")
    print(f"   Total wall time: {elapsed:.3f}s (Python ë ˆë²¨, ëª¨ë“  ì˜¤ë²„í—¤ë“œ í¬í•¨)")
    if total_db_execution_time > 0:
        print(f"   DB execution time: {total_db_execution_time:.3f}s (ìˆœìˆ˜ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„)")
        print(f"   Overhead: {elapsed - total_db_execution_time:.3f}s (AI ì¶”ë¡  + ë°ì´í„° ìˆ˜ì§‘)")
    else:
        print(f"   DB execution time: N/A (execution_time not collected)")
    
    result_file = save_test_result(algo_name, dataset_name, extra_info={
        "params": params,
        "num_queries": len(test_sqls),
        "wall_time": elapsed,
        "db_execution_time": total_db_execution_time,
        "overhead_time": elapsed - total_db_execution_time if total_db_execution_time > 0 else 0
    })
    
    
    # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (AI ì•Œê³ ë¦¬ì¦˜ì¸ ê²½ìš°)
    if algo_name != "baseline" and hasattr(scheduler, 'pilot_model'):
        from pilotscope.ModelRegistry import ModelRegistry
        
        model = scheduler.pilot_model
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance = {
            "total_time": elapsed,
            "average_time": elapsed / len(test_sqls) if test_sqls else 0,
            "num_queries": len(test_sqls)
        }
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        model.add_test_result(dataset_name, len(test_sqls), performance)
        
        # ëª¨ë¸ ì €ì¥ (ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸)
        model.save_model()
        
        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
        registry = ModelRegistry()
        registry.register_model(model.metadata)
        
        print(f"âœ… Model metadata saved: {model.model_id}")
    
    print(f"\nâœ… Test completed in {elapsed:.2f}s")
    print(f"   Result: {result_file}")
    
    return {
        "algorithm": algo_name,
        "dataset": dataset_name,
        "result_file": str(result_file),
        "elapsed_time": elapsed,
        "params": params
    }


def run_multiple_tests(config: PilotConfig, algorithms: List[str], 
                      datasets: List[str], algo_params: Dict = None) -> List[Dict]:
    """
    ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ + ë°ì´í„°ì…‹ ì¡°í•©ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    
    Args:
        config: PilotConfig ì¸ìŠ¤í„´ìŠ¤
        algorithms: í…ŒìŠ¤íŠ¸í•  ì•Œê³ ë¦¬ì¦˜ ë¦¬ìŠ¤íŠ¸
        datasets: í…ŒìŠ¤íŠ¸í•  ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
        algo_params: ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° (algo_nameì„ í‚¤ë¡œ í•˜ëŠ” dict)
    
    Returns:
        List[Dict]: ê° í…ŒìŠ¤íŠ¸ì˜ ê²°ê³¼ ì •ë³´
    """
    results = []
    
    for dataset in datasets:
        config.db = dataset
        
        for algo in algorithms:
            # ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            params = algo_params.get(algo, {}) if algo_params else {}
            
            scheduler = None
            try:
                result = run_single_test(config, algo, dataset, params)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"\nâŒ Test failed for {algo} on {dataset}: {e}")
                import traceback
                traceback.print_exc()
            finally:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë” ì² ì €í•˜ê²Œ
                print(f"\nğŸ§¹ Cleaning up resources for {algo} on {dataset}...")
                try:
                    pilotscope_exit()
                except Exception as e:
                    print(f"âš ï¸  Error during cleanup: {e}")
                
                # ê°•ì œë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                import gc
                gc.collect()
                
                # ì§§ì€ ëŒ€ê¸° ì‹œê°„ (DB ì—°ê²° ì™„ì „íˆ ë‹«íˆë„ë¡)
                import time
                time.sleep(2)
    
    return results


# ============================================================================
# Comparison & Report
# ============================================================================

def compare_results(results: List[Dict], output_dir: str = "results"):
    """
    ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¹„êµ
    
    Args:
        results: run_multiple_tests()ì˜ ë°˜í™˜ê°’
        output_dir: ë¹„êµ ì°¨íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    if len(results) < 2:
        print("\nâš ï¸  Need at least 2 results to compare")
        return
    
    print("\n" + "=" * 60)
    print("Comparison Summary")
    print("=" * 60)
    
    # ê²°ê³¼ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    result_files = [r["result_file"] for r in results]
    
    # ë¹„êµ ì‹¤í–‰
    output_path = f"{output_dir}/comparison_all"
    compare_algorithms(result_files, metric='total_time', output_path=output_path)
    
    # ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š Test Results:")
    print("-" * 60)
    for result in sorted(results, key=lambda x: x["elapsed_time"]):
        print(f"  {result['algorithm']:10s} on {result['dataset']:15s}: {result['elapsed_time']:8.2f}s")
    print("-" * 60)


# ============================================================================
# JSON Config File Support
# ============================================================================

def load_config_file(config_file: str) -> Dict:
    """JSON config íŒŒì¼ ë¡œë“œ"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_from_config_file(config_file: str):
    """
    JSON config íŒŒì¼ì—ì„œ ì‹¤í—˜ ì„¤ì •ì„ ì½ì–´ ì‹¤í–‰
    
    Config íŒŒì¼ í˜•ì‹:
    {
      "db_config": {...},
      "experiments": [
        {
          "name": "exp1",
          "algorithm": "mscn",
          "dataset": "stats_tiny",
          "params": {...}
        }
      ],
      "comparison": {...}
    }
    """
    print(f"\nğŸ“„ Loading config from: {config_file}")
    config_data = load_config_file(config_file)
    
    # DB Config ìƒì„±
    db_config_data = config_data.get("db_config", {})
    config = PostgreSQLConfig(**db_config_data)
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiments = config_data.get("experiments", [])
    results = []
    
    for exp in experiments:
        exp_name = exp.get("name", "unnamed")
        algo = exp.get("algorithm")
        dataset = exp.get("dataset")
        params = exp.get("params", {})
        
        print(f"\nğŸ§ª Running experiment: {exp_name}")
        config.db = dataset
        
        result = run_single_test(config, algo, dataset, params)
        if result:
            result["experiment_name"] = exp_name
            results.append(result)
        
        pilotscope_exit()
    
    # ë¹„êµ
    comparison_config = config_data.get("comparison", {})
    if comparison_config.get("enabled", True):
        compare_results(results, 
                       output_dir=comparison_config.get("output_dir", "results"))
    
    return results


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Unified test framework for multiple algorithms and datasets',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test multiple algorithms on multiple datasets
  python unified_test.py --algo mscn lero baseline --db stats_tiny production --compare
  
  # Test with custom parameters
  python unified_test.py --algo mscn --db production --epochs 50 --training-size 500
  
  # Use JSON config file
  python unified_test.py --config test_configs/production_experiment.json
        """
    )
    
    # Mode selection
    parser.add_argument('--config', help='JSON config file path')
    
    # Algorithm & Dataset selection
    parser.add_argument('--algo', nargs='+', 
                       choices=list(ALGORITHM_REGISTRY.keys()),
                       help='Algorithms to test')
    parser.add_argument('--db', nargs='+', 
                       help='Datasets to test (e.g., stats_tiny, imdb, production)')
    
    # Algorithm parameters
    parser.add_argument('--epochs', type=int, help='Number of training epochs')
    parser.add_argument('--training-size', type=int, 
                       help='Number of queries to use for training (-1 for all)')
    parser.add_argument('--collection-size', type=int,
                       help='Number of queries to collect for training (-1 for all)')
    parser.add_argument('--no-collection', action='store_true',
                       help='Disable data collection (use existing data)')
    parser.add_argument('--no-training', action='store_true',
                       help='Disable model training (use existing model)')
    
    # Output options
    parser.add_argument('--compare', action='store_true',
                       help='Compare results after all tests')
    parser.add_argument('--output-dir', default='results',
                       help='Output directory for results (default: results)')
    
    # DB Config
    parser.add_argument('--db-host', help='Database host')
    parser.add_argument('--db-port', help='Database port')
    parser.add_argument('--db-user', help='Database user')
    parser.add_argument('--db-pwd', help='Database password')
    parser.add_argument('--timeout', type=int, 
                       help='Request timeout in seconds (default: 600, recommend 900+ for Lero)')
    
    args = parser.parse_args()
    
    # JSON Config íŒŒì¼ ëª¨ë“œ
    if args.config:
        run_from_config_file(args.config)
        return
    
    # CLI ëª¨ë“œ
    if not args.algo or not args.db:
        parser.print_help()
        print("\nâŒ Error: --algo and --db are required (or use --config)")
        return
    
    # DB Config ìƒì„±
    config = PostgreSQLConfig()
    if args.db_host:
        config.db_host = args.db_host
    if args.db_port:
        config.db_port = args.db_port
    if args.db_user:
        config.db_user = args.db_user
    if args.db_pwd:
        config.db_user_pwd = args.db_pwd
    if args.timeout:
        config.once_request_timeout = args.timeout
        config.sql_execution_timeout = args.timeout
        print(f"â±ï¸  íƒ€ì„ì•„ì›ƒ ì„¤ì •: {args.timeout}ì´ˆ")
    
    # ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„° ì„¤ì •
    algo_params = {}
    for algo in args.algo:
        params = {}
        
        if args.epochs is not None:
            params['num_epoch'] = args.epochs
        if args.training_size is not None:
            params['num_training'] = args.training_size
        if args.collection_size is not None:
            params['num_collection'] = args.collection_size
        if args.no_collection:
            params['enable_collection'] = False
        if args.no_training:
            params['enable_training'] = False
        
        algo_params[algo] = params
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        results = run_multiple_tests(config, args.algo, args.db, algo_params)
        
        # ë¹„êµ
        if args.compare and len(results) > 1:
            compare_results(results, args.output_dir)
        
        print("\n" + "=" * 60)
        print("âœ¨ All tests completed!")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        pilotscope_exit()


if __name__ == '__main__':
    main()

