#!/usr/bin/env python3
"""
GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë° ì„±ëŠ¥ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys

def check_gpu():
    """GPU ë° CUDA ì„¤ì • í™•ì¸"""
    
    print("\n" + "="*60)
    print("GPU ë° CUDA ì„¤ì • í™•ì¸")
    print("="*60)
    
    # PyTorch í™•ì¸
    try:
        import torch
        print(f"\nâœ… PyTorch ë²„ì „: {torch.__version__}")
        
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        cuda_available = torch.cuda.is_available()
        print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ… YES' if cuda_available else 'âŒ NO (CPUë§Œ ì‚¬ìš©)'}")
        
        if cuda_available:
            print(f"   CUDA ë²„ì „: {torch.version.cuda}")
            print(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            
            # ê° GPU ì •ë³´
            for i in range(torch.cuda.device_count()):
                print(f"\n   ğŸ“Š GPU {i}:")
                print(f"      ì´ë¦„: {torch.cuda.get_device_name(i)}")
                print(f"      ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"      ì‚¬ìš© ì¤‘: {allocated:.2f} GB")
                    print(f"      ì˜ˆì•½ë¨: {cached:.2f} GB")
            
            # ê°„ë‹¨í•œ GPU í…ŒìŠ¤íŠ¸
            print(f"\n   ğŸ§ª GPU í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                x = torch.randn(1000, 1000).cuda()
                y = torch.randn(1000, 1000).cuda()
                z = torch.matmul(x, y)
                print(f"   âœ… GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            except Exception as e:
                print(f"   âŒ GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            print(f"\n   ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
            print(f"      1. NVIDIA GPUê°€ ìˆëŠ”ì§€ í™•ì¸")
            print(f"      2. NVIDIA Driver ì„¤ì¹˜")
            print(f"      3. nvidia-docker2 ì„¤ì¹˜")
            print(f"      4. docker-compose.ymlì— GPU ì„¤ì • í™•ì¸")
            print(f"      5. Docker ì¬ì‹œì‘")
    
    except ImportError:
        print(f"\nâŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   ì„¤ì¹˜: conda install pytorch torchvision -c pytorch")
        return False
    
    # TensorFlow í™•ì¸ (ì„ íƒì )
    print(f"\n" + "-"*60)
    try:
        import tensorflow as tf
        print(f"âœ… TensorFlow ë²„ì „: {tf.__version__}")
        gpus = tf.config.list_physical_devices('GPU')
        print(f"   GPU ê°œìˆ˜: {len(gpus)}")
        for gpu in gpus:
            print(f"   - {gpu}")
    except ImportError:
        print(f"âš ï¸  TensorFlowëŠ” ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ì„ íƒì )")
    except Exception as e:
        print(f"âš ï¸  TensorFlow GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("="*60 + "\n")
    return cuda_available if 'cuda_available' in locals() else False


def benchmark_performance():
    """GPU vs CPU ì„±ëŠ¥ ë¹„êµ"""
    try:
        import torch
        import time
        
        print("\n" + "="*60)
        print("GPU vs CPU ì„±ëŠ¥ ë¹„êµ")
        print("="*60)
        
        size = 5000
        iterations = 10
        
        # CPU í…ŒìŠ¤íŠ¸
        print(f"\nğŸ–¥ï¸  CPU í…ŒìŠ¤íŠ¸ (í–‰ë ¬ ê³±ì…ˆ {size}x{size}, {iterations}íšŒ)...")
        x_cpu = torch.randn(size, size)
        y_cpu = torch.randn(size, size)
        
        start = time.time()
        for _ in range(iterations):
            z_cpu = torch.matmul(x_cpu, y_cpu)
        cpu_time = time.time() - start
        print(f"   ì†Œìš” ì‹œê°„: {cpu_time:.2f}ì´ˆ ({cpu_time/iterations:.3f}ì´ˆ/íšŒ)")
        
        # GPU í…ŒìŠ¤íŠ¸
        if torch.cuda.is_available():
            print(f"\nğŸš€ GPU í…ŒìŠ¤íŠ¸ (í–‰ë ¬ ê³±ì…ˆ {size}x{size}, {iterations}íšŒ)...")
            x_gpu = torch.randn(size, size).cuda()
            y_gpu = torch.randn(size, size).cuda()
            
            # Warm-up
            z_gpu = torch.matmul(x_gpu, y_gpu)
            torch.cuda.synchronize()
            
            start = time.time()
            for _ in range(iterations):
                z_gpu = torch.matmul(x_gpu, y_gpu)
            torch.cuda.synchronize()
            gpu_time = time.time() - start
            print(f"   ì†Œìš” ì‹œê°„: {gpu_time:.2f}ì´ˆ ({gpu_time/iterations:.3f}ì´ˆ/íšŒ)")
            
            # ë¹„êµ
            speedup = cpu_time / gpu_time
            print(f"\nğŸ“Š ê²°ê³¼:")
            print(f"   GPU ì†ë„ í–¥ìƒ: {speedup:.1f}x ë¹ ë¦„")
            
            if speedup > 10:
                print(f"   âœ… GPUê°€ ë§¤ìš° íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
            elif speedup > 3:
                print(f"   âœ… GPUê°€ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            else:
                print(f"   âš ï¸  GPU ì„±ëŠ¥ì´ ì˜ˆìƒë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤. ë“œë¼ì´ë²„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            print(f"\nâš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜: {e}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    cuda_available = check_gpu()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì—¬ë¶€ ë¬¼ì–´ë³´ê¸°
    if len(sys.argv) > 1 and sys.argv[1] == '--benchmark':
        benchmark_performance()
    else:
        print("ğŸ’¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´:")
        print("   python check_gpu.py --benchmark")
        print()


if __name__ == '__main__':
    main()

