#!/usr/bin/env python3
"""
PostgreSQL ë¡œê·¸ íŒŒì¼ì—ì„œ SQL ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•˜ê³  train/test ë¶„í• 

ì‚¬ìš©ë²•:
    python extract_queries_from_log.py \
        --input /var/log/postgresql/postgresql.log \
        --output pilotscope/Dataset/Production/ \
        --train-ratio 0.8
"""

import argparse
import re
import os
from collections import Counter
from pathlib import Path


def parse_postgresql_log(log_file_path):
    """
    PostgreSQL ë¡œê·¸ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ SQL ì¿¼ë¦¬ ì¶”ì¶œ
    
    Args:
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        List[str]: ì¶”ì¶œëœ SQL ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    queries = []
    
    # PostgreSQL ë¡œê·¸ íŒ¨í„´
    # ì˜ˆ: 2024-01-01 10:00:00.000 UTC [12345] LOG:  statement: SELECT * FROM ...
    statement_pattern = re.compile(r'(?:LOG|STATEMENT):\s+(?:statement:|execute\s+\w+:)\s*(.+)', re.IGNORECASE)
    
    current_query = None
    
    with open(log_file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            # ìƒˆë¡œìš´ ì¿¼ë¦¬ ì‹œì‘
            match = statement_pattern.search(line)
            if match:
                if current_query:
                    queries.append(current_query.strip())
                current_query = match.group(1)
            elif current_query and not line.strip().startswith('['):
                # ë©€í‹°ë¼ì¸ ì¿¼ë¦¬ ê³„ì†
                current_query += ' ' + line.strip()
        
        # ë§ˆì§€ë§‰ ì¿¼ë¦¬ ì¶”ê°€
        if current_query:
            queries.append(current_query.strip())
    
    return queries


def filter_queries(queries, query_types=['SELECT']):
    """
    íŠ¹ì • íƒ€ì…ì˜ ì¿¼ë¦¬ë§Œ í•„í„°ë§
    
    Args:
        queries: ì „ì²´ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        query_types: í¬í•¨í•  ì¿¼ë¦¬ íƒ€ì… (ì˜ˆ: ['SELECT', 'INSERT'])
    
    Returns:
        List[str]: í•„í„°ë§ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    filtered = []
    
    for query in queries:
        query_upper = query.upper().strip()
        
        # ì¿¼ë¦¬ íƒ€ì… í™•ì¸
        is_target_type = any(query_upper.startswith(qtype) for qtype in query_types)
        
        # ì‹œìŠ¤í…œ ì¿¼ë¦¬ ì œì™¸
        is_system_query = any([
            'pg_catalog' in query_upper,
            'information_schema' in query_upper,
            'pg_stat' in query_upper,
            'pg_class' in query_upper,
            'pilotscope' in query_upper.lower(),  # PilotScope ë‚´ë¶€ ì¿¼ë¦¬ ì œì™¸
        ])
        
        if is_target_type and not is_system_query:
            filtered.append(query)
    
    return filtered


def normalize_query(query):
    """
    ì¿¼ë¦¬ ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±°, ê³µë°± ì •ë¦¬)
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
    
    Returns:
        str: ì •ê·œí™”ëœ ì¿¼ë¦¬
    """
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    query = re.sub(r'\s+', ' ', query)
    
    # ì£¼ì„ ì œê±°
    query = re.sub(r'--.*?$', '', query, flags=re.MULTILINE)
    query = re.sub(r'/\*.*?\*/', '', query, flags=re.DOTALL)
    
    # ì„¸ë¯¸ì½œë¡  ì œê±° (ë‚˜ì¤‘ì— ë‹¤ì‹œ ì¶”ê°€í•  ê²ƒ)
    query = query.rstrip(';')
    
    return query.strip()


def deduplicate_queries(queries, max_occurrences=10):
    """
    ì¤‘ë³µ ì¿¼ë¦¬ ì œê±° ë° ë¹ˆë„ ì œí•œ
    
    Args:
        queries: ì „ì²´ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        max_occurrences: ê° ì¿¼ë¦¬ì˜ ìµœëŒ€ ì¶œí˜„ íšŸìˆ˜
    
    Returns:
        List[str]: ì¤‘ë³µ ì œê±°ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    normalized = [normalize_query(q) for q in queries]
    
    # ë¹ˆë„ ê³„ì‚°
    query_counts = Counter(normalized)
    
    # ì¤‘ë³µ ì œê±°í•˜ë˜, ì¸ê¸° ìˆëŠ” ì¿¼ë¦¬ëŠ” max_occurrencesê¹Œì§€ ìœ ì§€
    result = []
    query_occurrence = {}
    
    for query in normalized:
        if query not in query_occurrence:
            query_occurrence[query] = 0
        
        if query_occurrence[query] < max_occurrences:
            result.append(query)
            query_occurrence[query] += 1
    
    return result


def split_train_test(queries, train_ratio=0.8, shuffle=True):
    """
    ì¿¼ë¦¬ë¥¼ train/testë¡œ ë¶„í• 
    
    Args:
        queries: ì „ì²´ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
        shuffle: ë¬´ì‘ìœ„ ì„ê¸° ì—¬ë¶€
    
    Returns:
        Tuple[List[str], List[str]]: (train_queries, test_queries)
    """
    import random
    
    queries_copy = queries.copy()
    
    if shuffle:
        random.shuffle(queries_copy)
    
    split_idx = int(len(queries_copy) * train_ratio)
    
    train_queries = queries_copy[:split_idx]
    test_queries = queries_copy[split_idx:]
    
    return train_queries, test_queries


def save_queries_to_file(queries, output_file):
    """
    ì¿¼ë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
    
    Args:
        queries: ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        for query in queries:
            # ê° ì¿¼ë¦¬ëŠ” ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ì¢…ë£Œ
            f.write(query + ';\n')
    
    print(f"âœ… Saved {len(queries)} queries to {output_file}")


def main():
    parser = argparse.ArgumentParser(description='Extract SQL queries from PostgreSQL log')
    parser.add_argument('--input', required=True, help='Input PostgreSQL log file path')
    parser.add_argument('--output', required=True, help='Output directory for train/test files')
    parser.add_argument('--train-ratio', type=float, default=0.8, help='Train/test split ratio (default: 0.8)')
    parser.add_argument('--query-types', nargs='+', default=['SELECT'], 
                       help='Query types to extract (default: SELECT)')
    parser.add_argument('--max-occurrences', type=int, default=10, 
                       help='Max occurrences for each unique query (default: 10)')
    parser.add_argument('--no-shuffle', action='store_true', help='Do not shuffle queries')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("PostgreSQL Query Log Extractor")
    print("=" * 60)
    
    # Step 1: ë¡œê·¸ íŒŒì‹±
    print(f"\nğŸ“‚ Reading log file: {args.input}")
    queries = parse_postgresql_log(args.input)
    print(f"   Found {len(queries)} total statements")
    
    # Step 2: ì¿¼ë¦¬ í•„í„°ë§
    print(f"\nğŸ” Filtering {args.query_types} queries...")
    filtered = filter_queries(queries, args.query_types)
    print(f"   Filtered to {len(filtered)} queries")
    
    # Step 3: ì¤‘ë³µ ì œê±°
    print(f"\nğŸ—‘ï¸  Deduplicating (max {args.max_occurrences} occurrences per query)...")
    deduplicated = deduplicate_queries(filtered, args.max_occurrences)
    print(f"   Deduplicated to {len(deduplicated)} queries")
    
    if len(deduplicated) == 0:
        print("\nâŒ No queries found. Please check your log file.")
        return
    
    # Step 4: Train/Test ë¶„í• 
    print(f"\nâœ‚ï¸  Splitting train/test (ratio: {args.train_ratio})...")
    train_queries, test_queries = split_train_test(
        deduplicated, 
        train_ratio=args.train_ratio, 
        shuffle=not args.no_shuffle
    )
    print(f"   Train: {len(train_queries)} queries")
    print(f"   Test:  {len(test_queries)} queries")
    
    # Step 5: íŒŒì¼ ì €ì¥
    print(f"\nğŸ’¾ Saving to {args.output}")
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_file = output_dir / "production_train.txt"
    test_file = output_dir / "production_test.txt"
    
    save_queries_to_file(train_queries, train_file)
    save_queries_to_file(test_queries, test_file)
    
    print("\n" + "=" * 60)
    print("âœ¨ Extraction complete!")
    print("=" * 60)
    print(f"\nNext steps:")
    print(f"1. Create ProductionDataset class in pilotscope/Dataset/ProductionDataset.py")
    print(f"2. Add 'production' to load_test_sql() in algorithm_examples/utils.py")
    print(f"3. Run tests with config.db = 'production_db'")
    print("=" * 60)


if __name__ == '__main__':
    main()

