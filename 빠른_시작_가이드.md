# PilotScope ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ëª©í‘œ
ìš´ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì¿¼ë¦¬ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ì„¤ì •ì„ ì°¾ê¸°

---

## ğŸ“‹ ì¤€ë¹„ë¬¼

- [x] PostgreSQL 13.1 ì„¤ì¹˜ ë° ì‹¤í–‰
- [x] ìš´ì˜ DB ì ‘ê·¼ ê¶Œí•œ
- [x] PilotScope ì„¤ì¹˜
- [x] Python 3.8+

---

## ğŸš€ 5ë‹¨ê³„ë¡œ ì‹œì‘í•˜ê¸°

### Step 1: ìš´ì˜ ì¿¼ë¦¬ ë¡œê·¸ ì¶”ì¶œ (15ë¶„)

```bash
# 1-1. PostgreSQL ë¡œê·¸ í™œì„±í™” (ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆë‹¤ë©´ ìƒëµ)
psql -U postgres -c "ALTER SYSTEM SET log_statement = 'all';"
psql -U postgres -c "SELECT pg_reload_conf();"

# 1-2. í•˜ë£¨ ì •ë„ ë¡œê·¸ ìˆ˜ì§‘ ëŒ€ê¸°
# ë¡œê·¸ ìœ„ì¹˜ í™•ì¸: SHOW log_directory;

# 1-3. ë¡œê·¸ì—ì„œ SQL ì¶”ì¶œ
cd pilotscope
python scripts/extract_queries_from_log.py \
    --input /var/log/postgresql/postgresql.log \
    --output pilotscope/Dataset/Production/ \
    --train-ratio 0.8
```

**ê²°ê³¼ í™•ì¸:**
```bash
ls pilotscope/Dataset/Production/
# production_train.txt  <- í•™ìŠµìš© ì¿¼ë¦¬
# production_test.txt   <- í…ŒìŠ¤íŠ¸ìš© ì¿¼ë¦¬
```

---

### Step 2: ProductionDataset í´ë˜ìŠ¤ ìƒì„± (5ë¶„)

**íŒŒì¼ ìƒì„±**: `pilotscope/Dataset/ProductionDataset.py`

```python
from pilotscope.Dataset.BaseDataset import BaseDataset
from pilotscope.PilotEnum import DatabaseEnum

class ProductionDataset(BaseDataset):
    """
    ì‹¤ì œ ìš´ì˜ í™˜ê²½ì˜ ì¿¼ë¦¬ ì›Œí¬ë¡œë“œ
    """
    sub_dir = "Production"
    train_sql_file = "production_train.txt"
    test_sql_file = "production_test.txt"
    file_db_type = DatabaseEnum.POSTGRESQL
    
    def __init__(self, use_db_type: DatabaseEnum, created_db_name="production_db"):
        super().__init__(use_db_type, created_db_name)
        self.download_urls = None
```

---

### Step 3: Utilsì— ë“±ë¡ (5ë¶„)

**íŒŒì¼ ìˆ˜ì •**: `algorithm_examples/utils.py`

```python
# ê¸°ì¡´ import ì„¹ì…˜ì— ì¶”ê°€
from pilotscope.Dataset.ProductionDataset import ProductionDataset

# load_test_sql() í•¨ìˆ˜ì— ì¶”ê°€
def load_test_sql(db):
    if "stats_tiny" == db.lower():
        return StatsTinyDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    # ... ê¸°ì¡´ ì½”ë“œ ...
    elif "production" == db.lower():  # ì¶”ê°€!
        return ProductionDataset(DatabaseEnum.POSTGRESQL).read_test_sql()
    else:
        raise NotImplementedError

# load_training_sql() í•¨ìˆ˜ì—ë„ ë™ì¼í•˜ê²Œ ì¶”ê°€
def load_training_sql(db):
    if "stats_tiny" == db.lower():
        return StatsTinyDataset(DatabaseEnum.POSTGRESQL).read_train_sql()
    # ... ê¸°ì¡´ ì½”ë“œ ...
    elif "production" == db.lower():  # ì¶”ê°€!
        return ProductionDataset(DatabaseEnum.POSTGRESQL).read_train_sql()
    else:
        raise NotImplementedError
```

---

### Step 4: í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (30ë¶„ ~ ìˆ˜ì‹œê°„)

**ë°©ë²• A: ì»¤ë§¨ë“œë¼ì¸ìœ¼ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸**

```bash
cd test_example_algorithms

# Baseline (AI ì—†ìŒ) ì„±ëŠ¥ ì¸¡ì •
python unified_test.py --algo baseline --db production

# MSCN ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ë²„ì „)
python unified_test.py --algo mscn --db production \
    --epochs 50 \
    --training-size 500 \
    --collection-size 500

# ê²°ê³¼ ë¹„êµ
python unified_test.py --algo baseline mscn --db production --compare
```

**ë°©ë²• B: JSON Config íŒŒì¼ ì‚¬ìš© (ê¶Œì¥)**

1. `test_configs/production_experiment.json` íŒŒì¼ ìˆ˜ì •:

```json
{
  "db_config": {
    "db": "your_production_db_name",  // ì‹¤ì œ DB ì´ë¦„ìœ¼ë¡œ ë³€ê²½
    "db_host": "localhost",
    "db_port": "5432",
    "db_user": "postgres",
    "db_user_pwd": "your_password"  // ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ë³€ê²½
  },
  
  "experiments": [
    {
      "name": "baseline",
      "algorithm": "baseline",
      "dataset": "production"
    },
    {
      "name": "mscn_fast",
      "algorithm": "mscn",
      "dataset": "production",
      "params": {
        "enable_collection": true,
        "enable_training": true,
        "num_collection": 500,
        "num_training": 500,
        "num_epoch": 50
      }
    }
  ]
}
```

2. ì‹¤í–‰:

```bash
python unified_test.py --config test_configs/production_experiment.json
```

---

### Step 5: ê²°ê³¼ ë¶„ì„ (10ë¶„)

```bash
# ì €ì¥ëœ ê²°ê³¼ í™•ì¸
python ../algorithm_examples/compare_results.py --list

# ìµœì‹  ê²°ê³¼ ë¹„êµ
python ../algorithm_examples/compare_results.py --latest baseline mscn --db production

# ì°¨íŠ¸ í™•ì¸
ls results/comparison_*.png
ls img/*_production.png
```

**ê²°ê³¼ í•´ì„:**
```
============================================================
Comparison Results (total_time):
============================================================
  mscn           :    45.2341s  â† MSCN ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
  baseline       :    67.8912s  â† AI ì—†ì´ ê¸°ë³¸ DB ìµœì í™”
============================================================
```

â†’ ì´ ê²½ìš° MSCNì´ **33% ì„±ëŠ¥ í–¥ìƒ** (67.89 â†’ 45.23ì´ˆ)

---

## ğŸ”¥ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

```bash
# Baseline, MSCN, Lero ëª¨ë‘ í…ŒìŠ¤íŠ¸
python unified_test.py \
    --algo baseline mscn lero \
    --db production \
    --compare \
    --epochs 100 \
    --training-size 1000
```

### Cross-Dataset Training

```python
# test_cross_dataset_training.py ìƒì„±
from pilotscope.PilotConfig import PostgreSQLConfig
from algorithm_examples.Mscn.MscnPresetScheduler import get_mscn_preset_scheduler
from algorithm_examples.utils import load_test_sql, save_test_result
from pilotscope.Common.TimeStatistic import TimeStatistic

config = PostgreSQLConfig()
config.db = "production_db"

# IMDB ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ìš´ì˜ DBì— ì ìš©
scheduler = get_mscn_preset_scheduler(
    config,
    enable_collection=True,
    enable_training=True,
    training_dataset="imdb",  # í•™ìŠµì€ IMDB ë°ì´í„°ë¡œ
    num_epoch=100
)

# ìš´ì˜ DBì—ì„œ í…ŒìŠ¤íŠ¸
sqls = load_test_sql("production")
for i, sql in enumerate(sqls):
    if i % 10 == 0:
        print(f"Progress: {i}/{len(sqls)}")
    TimeStatistic.start('MSCN_IMDB_Trained')
    scheduler.execute(sql)
    TimeStatistic.end('MSCN_IMDB_Trained')

save_test_result("mscn_imdb_trained", "production")
```

```bash
python test_cross_dataset_training.py
```

### Config Sweep (ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°)

```bash
# ì—¬ëŸ¬ epoch ì¡°í•© í…ŒìŠ¤íŠ¸
for epoch in 50 100 200; do
    for train_size in 500 1000 2000; do
        echo "Testing: epoch=$epoch, train_size=$train_size"
        python unified_test.py \
            --algo mscn \
            --db production \
            --epochs $epoch \
            --training-size $train_size
    done
done

# ê²°ê³¼ ë¹„êµ
python ../algorithm_examples/compare_results.py --list
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### ì‹œë‚˜ë¦¬ì˜¤ 1: E-commerce ì• í”Œë¦¬ì¼€ì´ì…˜
```
ì•Œê³ ë¦¬ì¦˜         | ì‹¤í–‰ ì‹œê°„ | ê°œì„ ìœ¨
----------------|----------|-------
Baseline        | 120.5s   | -
MSCN (ê¸°ë³¸)      | 89.3s    | 26% â†‘
MSCN (íŠœë‹)      | 75.2s    | 38% â†‘
Lero            | 82.1s    | 32% â†‘
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Analytics Dashboard
```
ì•Œê³ ë¦¬ì¦˜         | ì‹¤í–‰ ì‹œê°„ | ê°œì„ ìœ¨
----------------|----------|-------
Baseline        | 245.8s   | -
MSCN            | 198.4s   | 19% â†‘
Lero            | 176.3s   | 28% â†‘  â† ë³µì¡í•œ JOINì— ê°•í•¨
```

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### Q1: "No module named 'ProductionDataset'" ì—ëŸ¬
```bash
# utils.pyì— import ì¶”ê°€ í™•ì¸
grep "ProductionDataset" algorithm_examples/utils.py
```

### Q2: í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
```python
# íŒŒë¼ë¯¸í„° ì¡°ì •:
# - num_collection=500  # ìˆ˜ì§‘ ë°ì´í„° ì œí•œ
# - num_training=500    # í•™ìŠµ ë°ì´í„° ì œí•œ
# - num_epoch=50        # Epoch ì¤„ì´ê¸°
```

### Q3: ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ ì²˜ë¦¬:
sqls = load_test_sql("production")
for i in range(0, len(sqls), 100):  # 100ê°œì”© ì²˜ë¦¬
    batch = sqls[i:i+100]
    for sql in batch:
        scheduler.execute(sql)
```

### Q4: DB ì—°ê²° ì‹¤íŒ¨
```python
# pilotscope_conf.json í™•ì¸
cat pilotscope/pilotscope_conf.json

# ë˜ëŠ” ì½”ë“œì—ì„œ ì§ì ‘ ì„¤ì •
config = PostgreSQLConfig()
config.db_host = "your_host"
config.db_port = "5432"
config.db_user = "your_user"
config.db_user_pwd = "your_password"
```

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

### 1. ì •ë°€ íŠœë‹
- Config Sweepìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
- Cross-validationìœ¼ë¡œ ê³¼ì í•© ë°©ì§€

### 2. í”„ë¡œë•ì…˜ ì ìš©
```python
# ìµœì  ëª¨ë¸ ì„ íƒ í›„
scheduler = get_mscn_preset_scheduler(
    config,
    enable_collection=False,  # ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
    enable_training=False,
    num_epoch=100  # ìµœì ê°’
)
```

### 3. ì£¼ê¸°ì  ì¬í•™ìŠµ
```python
# ì£¼ê¸°ì ìœ¼ë¡œ ìƒˆ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
from algorithm_examples.Lero.LeroPresetScheduler import get_lero_dynamic_preset_scheduler

scheduler = get_lero_dynamic_preset_scheduler(config)
# 100ê°œ ì¿¼ë¦¬ë§ˆë‹¤ ìë™ ì¬í•™ìŠµ
```

### 4. ëª¨ë‹ˆí„°ë§
```bash
# ì„±ëŠ¥ ë³€í™” ì¶”ì 
watch -n 60 'python compare_results.py --latest baseline mscn'
```

---

## ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ìš´ì˜ ì¿¼ë¦¬ ë¡œê·¸ ì¶”ì¶œ ì™„ë£Œ
- [ ] ProductionDataset í´ë˜ìŠ¤ ìƒì„±
- [ ] utils.pyì— production dataset ë“±ë¡
- [ ] Baseline ì„±ëŠ¥ ì¸¡ì •
- [ ] MSCN/Lero í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ê²°ê³¼ ë¹„êµ ë° Best Config ì„ íƒ
- [ ] í”„ë¡œë•ì…˜ ì ìš© ê³„íš ìˆ˜ë¦½

---

## ğŸ’¡ ê¿€íŒ

### 1. ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
```bash
# ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
python unified_test.py --algo mscn --db production \
    --collection-size 100 \
    --training-size 100 \
    --epochs 10
```

### 2. GPU ì‚¬ìš© (Lero)
```python
import torch
print(f"GPU: {torch.cuda.is_available()}")  # Trueë©´ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©
```

### 3. ê²°ê³¼ ì¶”ì 
```bash
# Gitìœ¼ë¡œ ì‹¤í—˜ ê²°ê³¼ ë²„ì „ ê´€ë¦¬
git add results/*.json test_configs/*.json
git commit -m "Experiment: MSCN on production (epoch=100)"
```

### 4. ë³‘ë ¬ ì‹¤í–‰
```bash
# ì—¬ëŸ¬ í„°ë¯¸ë„ì—ì„œ ë™ì‹œ ì‹¤í–‰
# Terminal 1
python unified_test.py --algo mscn --db production &

# Terminal 2
python unified_test.py --algo lero --db production &

# ì™„ë£Œ í›„ ë¹„êµ
wait
python compare_results.py --latest mscn lero
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **ìƒì„¸ ë¬¸ì„œ**: `ìš´ì˜_ë°ì´í„°_ê¸°ë°˜_ìµœì í™”_ê°€ì´ë“œ.md`
- **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹**: `algorithm_examples/CUSTOM_DATASET_GUIDE.md`
- **ê²°ê³¼ ê´€ë¦¬**: `algorithm_examples/README_RESULTS.md`
- **PilotScope ë…¼ë¬¸**: `paper/PilotScope.pdf`

---

**ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ì˜¬ë¦¬ê±°ë‚˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”! ğŸš€**

